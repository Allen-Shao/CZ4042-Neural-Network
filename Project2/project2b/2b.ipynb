{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load import mnist\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 encoder, decoder and a softmax layer\n",
    "\n",
    "def init_weights(n_visible, n_hidden):\n",
    "    initial_W = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            size=(n_visible, n_hidden)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "def init_bias(n):\n",
    "    return theano.shared(value=np.zeros(n,dtype=theano.config.floatX),borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mnist_data(trainX, corrupted, testX, reconstructed, tag):\n",
    "    # plot 100 original data\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(trainX[i,:].reshape(28,28))\n",
    "    pylab.savefig('./Graph/' + tag + '_original.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    # plot 100 corrupted data\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(corrupted[i,:].reshape(28,28))\n",
    "    pylab.savefig('./Graph/' + tag + '_corrupted.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    # plot 100 tested data\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(testX[i,:].reshape(28,28))\n",
    "    pylab.savefig('./Graph/' + tag + '_tested.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    # plot 100 reconstructed data\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(reconstructed[i,:].reshape(28,28))\n",
    "    pylab.savefig('./Graph/' + tag + '_reconstructed.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    print('plot_mnist_data finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weight(weight, tag):\n",
    "    # Plot 100 samples of weights (as images) learned at each layer\n",
    "    w = weight.get_value()\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(w[:,i].reshape(28,28))\n",
    "    pylab.savefig('./Graph/' + tag + '_weight.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    print('plot_weight finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_traning_error(d, tag):\n",
    "    global training_epochs\n",
    "    pylab.figure()\n",
    "    pylab.plot(range(training_epochs), d)\n",
    "    pylab.xlabel('iterations')\n",
    "    pylab.ylabel('cross-entropy training error')\n",
    "    pylab.savefig('./Graph/' + tag + '_training_error.png')\n",
    "    pylab.close()\n",
    "    \n",
    "def plot_test_accuracy(acc, tag):\n",
    "    global training_epochs\n",
    "    pylab.figure()\n",
    "    pylab.plot(range(training_epochs), acc)\n",
    "    pylab.xlabel('iterations')\n",
    "    pylab.ylabel('test accuracy')\n",
    "    pylab.savefig('./Graph/' + tag + '_test_acc.png')\n",
    "    pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "trX, trY = trX[:12000], trY[:12000]\n",
    "teX, teY = teX[:2000], teY[:2000]\n",
    "\n",
    "print(trX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question B (1) & B(2)\n",
    "# construct the network\n",
    "def construct_nn_part1_2():\n",
    "    x = T.fmatrix('x')  \n",
    "    d = T.fmatrix('d')\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    corruption_level=0.1\n",
    "    learning_rate = 0.1\n",
    "\n",
    "    no_hidden1 = 900\n",
    "    no_hidden2 = 625\n",
    "    no_hidden3 = 400\n",
    "\n",
    "    # 3 layers for encoder\n",
    "    W1, b1 = init_weights(28*28, no_hidden1) , init_bias(no_hidden1)\n",
    "    W2, b2 = init_weights(no_hidden1, no_hidden2), init_bias(no_hidden2)\n",
    "    W3, b3 = init_weights(no_hidden2, no_hidden3), init_bias(no_hidden3)\n",
    "    W4, b4 = init_weights(no_hidden3, 10), init_bias(10) # output layer for question B(2)\n",
    "\n",
    "    # 3 layers for decoder\n",
    "    b1_prime = init_bias(28*28)\n",
    "    W1_prime = W1.transpose() \n",
    "    b2_prime = init_bias(no_hidden1)\n",
    "    W2_prime = W2.transpose()\n",
    "    b3_prime = init_bias(no_hidden2)\n",
    "    W3_prime = W3.transpose()\n",
    "\n",
    "    tilde_x = theano_rng.binomial(size=x.shape, n=1, p=1 - corruption_level,\n",
    "                                  dtype=theano.config.floatX)*x\n",
    "    # 3 layers for encoder\n",
    "    y1 = T.nnet.sigmoid(T.dot(tilde_x, W1) + b1)\n",
    "    y2 = T.nnet.sigmoid(T.dot(y1, W2) + b2)\n",
    "    y3 = T.nnet.sigmoid(T.dot(y2, W3) + b3)\n",
    "\n",
    "    # 3 layers for decoder\n",
    "    z1 = T.nnet.sigmoid(T.dot(y3, W3_prime) + b3_prime)\n",
    "    z2 = T.nnet.sigmoid(T.dot(z1, W2_prime) + b2_prime)\n",
    "    z3 = T.nnet.sigmoid(T.dot(z2, W1_prime) + b1_prime)\n",
    "\n",
    "#     crossentropy(py, Y))\n",
    "    cost_da = - T.mean(T.sum(x * T.log(z3) + (1 - x) * T.log(1 - z3), axis=1))\n",
    "\n",
    "    params_da = [W1, b1, W2, b2, W3, b3, b1_prime, b2_prime, b3_prime]\n",
    "    grads_da = T.grad(cost_da, params_da)\n",
    "    updates_da = [(param_da, param_da - learning_rate * grad_da)\n",
    "               for param_da, grad_da in zip(params_da, grads_da)]\n",
    "    train_da = theano.function(inputs=[x], outputs = cost_da, updates = updates_da, allow_input_downcast = True)\n",
    "    test_da = theano.function(inputs=[x], outputs = z3, allow_input_downcast = True)\n",
    "    \n",
    "    # five-layer feedforward neuron network\n",
    "    output_ff = T.nnet.softmax(T.dot(y3, W4)+b4)\n",
    "    predicted_result_ff = T.argmax(output_ff, axis=1)\n",
    "    cost_ff = T.mean(T.nnet.categorical_crossentropy(output_ff, d))\n",
    "\n",
    "    params_ff = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "    grads_ff = T.grad(cost_ff, params_ff)\n",
    "    updates_ff = [(param_ff, param_ff - learning_rate * grad_ff)\n",
    "               for param_ff, grad_ff in zip(params_ff, grads_ff)]\n",
    "    noisy_data = theano.function(inputs=[x], outputs = tilde_x, allow_input_downcast = True)\n",
    "    train_ffn = theano.function(inputs=[x, d], outputs = cost_ff, updates = updates_ff, allow_input_downcast = True)\n",
    "    test_ffn = theano.function(inputs=[x], outputs = predicted_result_ff, allow_input_downcast=True)\n",
    "    \n",
    "    return train_da, test_da, train_ffn, test_ffn, noisy_data, W1, W2, W3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_da, test_da, train_ffn, test_ffn, noisy_data, W1, W2, W3 = construct_nn_part1_2()\n",
    "print('training dae1 ...')\n",
    "training_epochs = 25\n",
    "batch_size = 128\n",
    "reconstruction_error = []\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    # go through trainng set\n",
    "    c = []\n",
    "    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "        c.append(train_da(trX[start:end])) # costs\n",
    "    reconstruction_error.append(np.mean(c, dtype='float64')) # reconstruction errors\n",
    "    print(reconstruction_error[epoch])\n",
    "    \n",
    "reconstructed_x = test_da(teX)\n",
    "tilde_x = noisy_data(trX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mnist_data(trX, tilde_x, teX, reconstructed_x, '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weight(W1, '1_W1')\n",
    "# do not know how to reshape\n",
    "# plot_weight(W2, '1_W2')  # w2 is in shape (900, 625)\n",
    "# plot_weight(W3, '1_W3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot learning curves (i.e., reconstruction errors on training data) for training each epoch\n",
    "plot_traning_error(reconstruction_error, '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntraining ffn ...')\n",
    "ff_training_cost, ff_acc = [], []\n",
    "for epoch in range(training_epochs):\n",
    "    # go through trainng set\n",
    "    c = []\n",
    "    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "        c.append(train_ffn(trX[start:end], trY[start:end]))\n",
    "    ff_training_cost.append(np.mean(c, dtype='float64')) # training cost\n",
    "    ff_acc.append(np.mean(np.argmax(teY, axis=1) == test_ffn(teX))) # accuracy\n",
    "    print(ff_acc[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_traning_error(ff_training_cost, '2')\n",
    "plot_test_accuracy(ff_acc, '2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
