{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load import mnist\n",
    "import numpy as np\n",
    "\n",
    "import pylab\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 encoder, decoder and a softmax layer\n",
    "\n",
    "def init_weights(n_visible, n_hidden):\n",
    "    initial_W = np.asarray(\n",
    "        np.random.uniform(\n",
    "            low=-4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            high=4 * np.sqrt(6. / (n_hidden + n_visible)),\n",
    "            size=(n_visible, n_hidden)),\n",
    "        dtype=theano.config.floatX)\n",
    "    return theano.shared(value=initial_W, name='W', borrow=True)\n",
    "\n",
    "def init_bias(n):\n",
    "    return theano.shared(value=np.zeros(n,dtype=theano.config.floatX),borrow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_mnist_data(X, file_name):\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    size = int(np.sqrt(X[0].shape[0]))\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(X[i,:].reshape(size,size))\n",
    "    pylab.savefig('./Graph/' + file_name)\n",
    "    pylab.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weight(weight, tag):\n",
    "    # Plot 100 samples of weights (as images) learned at each layer\n",
    "    w = weight.get_value()\n",
    "    pylab.figure()\n",
    "    pylab.gray()\n",
    "    size = int(np.sqrt(w.shape[0]))\n",
    "    for i in range(100):\n",
    "        pylab.subplot(10, 10, i+1); pylab.axis('off'); pylab.imshow(w[:,i].reshape(size,size))\n",
    "    pylab.savefig('./Graph/' + tag + '_weight.png')\n",
    "    pylab.close()\n",
    "    \n",
    "    print('plot_weight finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_training_error(d, tag):\n",
    "    global training_epochs\n",
    "    pylab.figure()\n",
    "    pylab.plot(range(training_epochs), d)\n",
    "    pylab.xlabel('iterations')\n",
    "    pylab.ylabel('cross-entropy training error')\n",
    "    pylab.savefig('./Graph/' + tag + '_training_error.png')\n",
    "    pylab.close()\n",
    "    \n",
    "def plot_test_accuracy(acc, tag):\n",
    "    global training_epochs\n",
    "    pylab.figure()\n",
    "    pylab.plot(range(training_epochs), acc)\n",
    "    pylab.xlabel('iterations')\n",
    "    pylab.ylabel('test accuracy')\n",
    "    pylab.savefig('./Graph/' + tag + '_test_acc.png')\n",
    "    pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "trX, teX, trY, teY = mnist()\n",
    "\n",
    "trX, trY = trX[:12000], trY[:12000]\n",
    "teX, teY = teX[:2000], teY[:2000]\n",
    "\n",
    "print(trX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# hyper-parameter\n",
    "corruption_level=0.1\n",
    "learning_rate = 0.1\n",
    "\n",
    "momentum = 0.1\n",
    "beta = 0.5 # penalty parameter\n",
    "rho = 0.05 # sparcity parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# question B (1) & B(2)\n",
    "# construct the network\n",
    "def construct_nn_part1_2():\n",
    "    x = T.fmatrix('x')  \n",
    "    d = T.fmatrix('d')\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    no_hidden1 = 900\n",
    "    no_hidden2 = 625\n",
    "    no_hidden3 = 400\n",
    "\n",
    "\n",
    "    W1, b1 = init_weights(28*28, no_hidden1) , init_bias(no_hidden1)\n",
    "    W2, b2 = init_weights(no_hidden1, no_hidden2), init_bias(no_hidden2)\n",
    "    W3, b3 = init_weights(no_hidden2, no_hidden3), init_bias(no_hidden3)\n",
    "    W4, b4 = init_weights(no_hidden3, 10), init_bias(10) # output layer for question B(2)\n",
    "\n",
    "\n",
    "    b1_prime = init_bias(28*28)\n",
    "    W1_prime = W1.transpose() # (900,784)\n",
    "    b2_prime = init_bias(no_hidden1)\n",
    "    W2_prime = W2.transpose() # (625, 900)\n",
    "    b3_prime = init_bias(no_hidden2)\n",
    "    W3_prime = W3.transpose() # (400,625)\n",
    "\n",
    "\n",
    "\n",
    "    #  train on the inputs tilde_x to learn primary features y1\n",
    "    tilde_x = theano_rng.binomial(size=x.shape, n=1, p=1 - corruption_level,\n",
    "                              dtype=theano.config.floatX)*x\n",
    "    y1 = T.nnet.sigmoid(T.dot(tilde_x, W1) + b1)\n",
    "    z1 = T.nnet.sigmoid(T.dot(y1, W1_prime) + b1_prime)\n",
    "    cost_da1 = - T.mean(T.sum(x * T.log(z1) + (1 - x) * T.log(1 - z1), axis=1))\n",
    "    params_da1 = [W1, b1, b1_prime]\n",
    "    grads_da1 = T.grad(cost_da1, params_da1)\n",
    "    updates_da1 = [(param_da, param_da - learning_rate * grad_da)\n",
    "               for param_da, grad_da in zip(params_da1, grads_da1)]\n",
    "    train_da1 = theano.function(inputs=[x], outputs = cost_da1, updates = updates_da1, allow_input_downcast = True)\n",
    "    \n",
    "    #  train on the inputs y1 to learn features y2\n",
    "    tilde_y1 = theano_rng.binomial(size=y1.shape, n=1, p=1 - corruption_level,\n",
    "                          dtype=theano.config.floatX)*y1\n",
    "    y2 = T.nnet.sigmoid(T.dot(tilde_y1, W2) + b2)\n",
    "    z2 = T.nnet.sigmoid(T.dot(y2, W2_prime) + b2_prime)\n",
    "    cost_da2 = - T.mean(T.sum(y1 * T.log(z2) + (1 - y1) * T.log(1 - z2), axis=1))\n",
    "    params_da2 = [W2, b2, b2_prime]\n",
    "    grads_da2 = T.grad(cost_da2, params_da2)\n",
    "    updates_da2 = [(param_da, param_da - learning_rate * grad_da)\n",
    "               for param_da, grad_da in zip(params_da2, grads_da2)]\n",
    "    train_da2 = theano.function(inputs=[x], outputs = cost_da2, updates = updates_da2, allow_input_downcast = True)\n",
    "\n",
    "    #  train on the inputs y2 to learn features y3\n",
    "    tilde_y2 = theano_rng.binomial(size=y2.shape, n=1, p=1 - corruption_level,\n",
    "                          dtype=theano.config.floatX)*y2\n",
    "    y3 = T.nnet.sigmoid(T.dot(tilde_y2, W3) + b3)\n",
    "    z3 = T.nnet.sigmoid(T.dot(y3, W3_prime) + b3_prime)\n",
    "    cost_da3 = - T.mean(T.sum(y2 * T.log(z3) + (1 - y2) * T.log(1 - z3), axis=1))\n",
    "    params_da3 = [W3, b3, b3_prime]\n",
    "    grads_da3 = T.grad(cost_da3, params_da3)\n",
    "    updates_da3 = [(param_da, param_da - learning_rate * grad_da)\n",
    "               for param_da, grad_da in zip(params_da3, grads_da3)]\n",
    "    train_da3 = theano.function(inputs=[x], outputs = cost_da3, updates = updates_da3, allow_input_downcast = True)\n",
    "    \n",
    "    encoder1 = theano.function(inputs=[x], outputs = y1, allow_input_downcast=True)\n",
    "    encoder2 = theano.function(inputs=[y1], outputs = y2, allow_input_downcast=True)\n",
    "    encoder3 = theano.function(inputs=[y2], outputs = y3, allow_input_downcast=True)\n",
    "    \n",
    "    decoder3 = theano.function(inputs=[y3],outputs = z3, allow_input_downcast=True) # 625\n",
    "    decoder2 = theano.function(inputs=[y2],outputs = z2, allow_input_downcast=True) # 900\n",
    "    decoder1 = theano.function(inputs=[y1], outputs = z1, allow_input_downcast=True) # 784\n",
    "\n",
    "    \n",
    "    # five-layer feedforward neuron network\n",
    "    output_ff = T.nnet.softmax(T.dot(y3, W4)+b4)\n",
    "    predicted_result_ff = T.argmax(output_ff, axis=1)\n",
    "    cost_ff = T.mean(T.nnet.categorical_crossentropy(output_ff, d))\n",
    "\n",
    "    params_ff = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "    grads_ff = T.grad(cost_ff, params_ff)\n",
    "    updates_ff = [(param_ff, param_ff - learning_rate * grad_ff)\n",
    "               for param_ff, grad_ff in zip(params_ff, grads_ff)]\n",
    "    noisy_data = theano.function(inputs=[x], outputs = tilde_x, allow_input_downcast = True)\n",
    "    train_ffn = theano.function(inputs=[x, d], outputs = cost_ff, updates = updates_ff, allow_input_downcast = True)\n",
    "    test_ffn = theano.function(inputs=[x], outputs = predicted_result_ff, allow_input_downcast=True)\n",
    "    \n",
    "    return [train_da1, train_da2, train_da3], [encoder1, encoder2, encoder3], [decoder3, decoder2, decoder1], train_ffn, test_ffn, W1, W2, W3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question B1\n",
    "train_da, encoder, decoder, train_ffn, test_ffn, W1, W2, W3 = construct_nn_part1_2()\n",
    "print('training dae1 ...')\n",
    "training_epochs = 25\n",
    "batch_size = 128\n",
    "reconstruction_error = []\n",
    "\n",
    "for i in range(3):\n",
    "    d = []\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "            c.append(train_da[i](trX[start:end])) # costs\n",
    "\n",
    "        d.append(np.mean(c, dtype='float64')) # reconstruction errors\n",
    "    \n",
    "    reconstruction_error.append(d)\n",
    "    print(\"Finished training layer %d\" % (i+1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_list = list(range(len(teX)))\n",
    "np.random.shuffle(id_list)\n",
    "test_id = id_list[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result plotting for question B1\n",
    "plot_mnist_data(trX, \"1/1_original\")\n",
    "plot_mnist_data(teX, \"1/1_test\")\n",
    "plot_weight(W1, '1/1_W1')\n",
    "plot_weight(W2, '1/1_W2')  \n",
    "plot_weight(W3, '1/1_W3')\n",
    "encoded_image = teX[test_id]\n",
    "for i in range(len(encoder)):\n",
    "    encoded_image = encoder[i](encoded_image)\n",
    "    plot_mnist_data(encoded_image, \"1/1_\" + str(i+1) + \"rd_hidden_layer_activation\")\n",
    "decoded_image = encoded_image\n",
    "for i in decoder:\n",
    "    decoded_image = i(decoded_image)\n",
    "plot_mnist_data(decoded_image, \"1/test_reconstructed\")\n",
    "\n",
    "# plot learning curves (i.e., reconstruction errors on training data) for training each epoch\n",
    "for i in range(len(reconstruction_error)):\n",
    "    plot_training_error(reconstruction_error[i], '1/1_' + str(i+1) + 'rd_layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question B2\n",
    "print('\\ntraining ffn ...')\n",
    "ff_training_cost, ff_acc = [], []\n",
    "for epoch in range(training_epochs):\n",
    "    # go through trainng set\n",
    "    c = []\n",
    "    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "        c.append(train_ffn(trX[start:end], trY[start:end]))\n",
    "    ff_training_cost.append(np.mean(c, dtype='float64')) # training cost\n",
    "    ff_acc.append(np.mean(np.argmax(teY, axis=1) == test_ffn(teX))) # accuracy\n",
    "    print(ff_acc[epoch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result plotting for question B2\n",
    "plot_training_error(ff_training_cost, '2/2')\n",
    "plot_test_accuracy(ff_acc, '2/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question B (3)\n",
    "# construct the network\n",
    "def sgd_momentum(cost, params, lr, momentum):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        v = theano.shared(p.get_value())\n",
    "#         v_new = momentum*v - (g + decay*p) * lr \n",
    "        v_new = momentum*v - g * lr \n",
    "        updates.append([p, p + v_new])\n",
    "        updates.append([v, v_new])\n",
    "        return updates\n",
    "    \n",
    "def construct_nn_part3():\n",
    "    x = T.fmatrix('x')  \n",
    "    d = T.fmatrix('d')\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    theano_rng = RandomStreams(rng.randint(2 ** 30))\n",
    "\n",
    "    no_hidden1 = 900\n",
    "    no_hidden2 = 625\n",
    "    no_hidden3 = 400\n",
    "\n",
    "    W1, b1 = init_weights(28*28, no_hidden1) , init_bias(no_hidden1)\n",
    "    W2, b2 = init_weights(no_hidden1, no_hidden2), init_bias(no_hidden2)\n",
    "    W3, b3 = init_weights(no_hidden2, no_hidden3), init_bias(no_hidden3)\n",
    "    W4, b4 = init_weights(no_hidden3, 10), init_bias(10) # output layer for question B(2)\n",
    "\n",
    "\n",
    "    b1_prime = init_bias(28*28)\n",
    "    W1_prime = W1.transpose() # (900,784)\n",
    "    b2_prime = init_bias(no_hidden1)\n",
    "    W2_prime = W2.transpose() # (625, 900)\n",
    "    b3_prime = init_bias(no_hidden2)\n",
    "    W3_prime = W3.transpose() # (400,625)\n",
    "\n",
    "    #  train on the inputs tilde_x to learn primary features y1\n",
    "    tilde_x = theano_rng.binomial(size=x.shape, n=1, p=1 - corruption_level,\n",
    "                                  dtype=theano.config.floatX)*x\n",
    "    y1 = T.nnet.sigmoid(T.dot(tilde_x, W1) + b1)\n",
    "    z1 = T.nnet.sigmoid(T.dot(y1, W1_prime) + b1_prime)\n",
    "    cost_da1 = - T.mean(T.sum(x * T.log(z1) + (1 - x) * T.log(1 - z1), axis=1)) \n",
    "    + beta*T.shape(y1)[1]*(rho*T.log(rho) + (1-rho)*T.log(1-rho)) \n",
    "    - beta*rho*T.sum(T.log(T.mean(y1, axis=0)+1e-6)) \n",
    "    - beta*(1-rho)*T.sum(T.log(1-T.mean(y1, axis=0)+1e-6))\n",
    "                \n",
    "    params_da1 = [W1, b1, b1_prime]\n",
    "    grads_da1 = T.grad(cost_da1, params_da1)\n",
    "#     updates_da1 = [(param_da, momentum * param_da - learning_rate * grad_da)\n",
    "#                for param_da, grad_da in zip(params_da1, grads_da1)]\n",
    "    train_da1 = theano.function(inputs=[x], outputs = cost_da1, updates = sgd_momentum(cost_da1, params_da1, learning_rate, momemtum), allow_input_downcast = True)\n",
    "    \n",
    "    #  train on the inputs y1 to learn features y2\n",
    "    tilde_y1 = theano_rng.binomial(size=y1.shape, n=1, p=1 - corruption_level,\n",
    "                                  dtype=theano.config.floatX)*y1\n",
    "    y2 = T.nnet.sigmoid(T.dot(tilde_y1, W2) + b2)\n",
    "    z2 = T.nnet.sigmoid(T.dot(y2, W2_prime) + b2_prime)\n",
    "    cost_da2 = - T.mean(T.sum(y1 * T.log(z2) + (1 - y1) * T.log(1 - z2), axis=1))\n",
    "    + beta*T.shape(y2)[1]*(rho*T.log(rho) + (1-rho)*T.log(1-rho)) \n",
    "    - beta*rho*T.sum(T.log(T.mean(y2, axis=0)+1e-6)) \n",
    "    - beta*(1-rho)*T.sum(T.log(1-T.mean(y2, axis=0)+1e-6))\n",
    "                \n",
    "    params_da2 = [W2, b2, b2_prime]\n",
    "    grads_da2 = T.grad(cost_da2, params_da2)\n",
    "#     updates_da2 = [(param_da, momentum * param_da - learning_rate * grad_da)\n",
    "#                for param_da, grad_da in zip(params_da2, grads_da2)]\n",
    "    train_da2 = theano.function(inputs=[x], outputs = cost_da2, updates = sgd_momentum(cost_da2, params_da2, learning_rate, momemtum), allow_input_downcast = True)\n",
    "\n",
    "    #  train on the inputs y2 to learn features y3\n",
    "    tilde_y2 = theano_rng.binomial(size=y2.shape, n=1, p=1 - corruption_level,\n",
    "                                  dtype=theano.config.floatX)*y2\n",
    "    y3 = T.nnet.sigmoid(T.dot(tilde_y2, W3) + b3)\n",
    "    z3 = T.nnet.sigmoid(T.dot(y3, W3_prime) + b3_prime)\n",
    "    cost_da3 = - T.mean(T.sum(y2 * T.log(z3) + (1 - y2) * T.log(1 - z3), axis=1))\n",
    "    + beta*T.shape(y3)[1]*(rho*T.log(rho) + (1-rho)*T.log(1-rho)) \n",
    "    - beta*rho*T.sum(T.log(T.mean(y3, axis=0)+1e-6)) \n",
    "    - beta*(1-rho)*T.sum(T.log(1-T.mean(y3, axis=0)+1e-6))\n",
    "                \n",
    "    params_da3 = [W3, b3, b3_prime]\n",
    "    grads_da3 = T.grad(cost_da3, params_da3)\n",
    "#     updates_da3 = [(param_da, momentum * param_da - learning_rate * grad_da)\n",
    "#                for param_da, grad_da in zip(params_da3, grads_da3)]\n",
    "    train_da3 = theano.function(inputs=[x], outputs = cost_da3, updates = sgd_momentum(cost_da3, params_da3, learning_rate, momemtum), allow_input_downcast = True)\n",
    "    \n",
    "    encoder1 = theano.function(inputs=[x], outputs = y1, allow_input_downcast=True)\n",
    "    encoder2 = theano.function(inputs=[y1], outputs = y2, allow_input_downcast=True)\n",
    "    encoder3 = theano.function(inputs=[y2], outputs = y3, allow_input_downcast=True)\n",
    "    \n",
    "    decoder3 = theano.function(inputs=[y3],outputs = z3, allow_input_downcast=True) # 625\n",
    "    decoder2 = theano.function(inputs=[y2],outputs = z2, allow_input_downcast=True) # 900\n",
    "    decoder1 = theano.function(inputs=[y1], outputs = z1, allow_input_downcast=True) # 784\n",
    "\n",
    "    \n",
    "    # five-layer feedforward neuron network\n",
    "    output_ff = T.nnet.softmax(T.dot(y3, W4)+b4)\n",
    "    predicted_result_ff = T.argmax(output_ff, axis=1)\n",
    "    cost_ff = T.mean(T.nnet.categorical_crossentropy(output_ff, d))\n",
    "\n",
    "    params_ff = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
    "    grads_ff = T.grad(cost_ff, params_ff)\n",
    "    updates_ff = [(param_ff, param_ff - learning_rate * grad_ff)\n",
    "               for param_ff, grad_ff in zip(params_ff, grads_ff)]\n",
    "    noisy_data = theano.function(inputs=[x], outputs = tilde_x, allow_input_downcast = True)\n",
    "    train_ffn = theano.function(inputs=[x, d], outputs = cost_ff, updates = updates_ff, allow_input_downcast = True)\n",
    "    test_ffn = theano.function(inputs=[x], outputs = predicted_result_ff, allow_input_downcast=True)\n",
    "    \n",
    "    return [train_da1, train_da2, train_da3], [encoder1, encoder2, encoder3], [decoder3, decoder2, decoder1], train_ffn, test_ffn, W1, W2, W3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question B3\n",
    "train_da, encoder, decoder, train_ffn, test_ffn, W1, W2, W3 = construct_nn_part1_2()\n",
    "print('training dae1 ...')\n",
    "training_epochs = 25\n",
    "batch_size = 128\n",
    "reconstruction_error = []\n",
    "\n",
    "for i in range(3):\n",
    "    d = []\n",
    "    for epoch in range(training_epochs):\n",
    "        # go through trainng set\n",
    "        c = []\n",
    "        for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "            c.append(train_da[i](trX[start:end])) # costs\n",
    "\n",
    "        d.append(np.mean(c, dtype='float64')) # reconstruction errors\n",
    "    \n",
    "    reconstruction_error.append(d)\n",
    "    print(\"Finished training layer %d\" % (i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result plotting for question B3\n",
    "plot_mnist_data(trX, \"3/1_original\")\n",
    "plot_mnist_data(teX, \"3/1_test\")\n",
    "plot_weight(W1, '3/1_W1')\n",
    "plot_weight(W2, '3/1_W2')  \n",
    "plot_weight(W3, '3/1_W3')\n",
    "encoded_image = teX[test_id]\n",
    "for i in range(len(encoder)):\n",
    "    encoded_image = encoder[i](encoded_image)\n",
    "    plot_mnist_data(encoded_image, \"3/1_\" + str(i+1) + \"rd_hidden_layer_activation\")\n",
    "decoded_image = encoded_image\n",
    "for i in decoder:\n",
    "    decoded_image = i(decoded_image)\n",
    "plot_mnist_data(decoded_image, \"3/test_reconstructed\")\n",
    "\n",
    "# plot learning curves (i.e., reconstruction errors on training data) for training each epoch\n",
    "for i in range(len(reconstruction_error)):\n",
    "    plot_training_error(reconstruction_error[i], '3/1_' + str(i+1) + 'rd_layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntraining ffn ...')\n",
    "ff_training_cost, ff_acc = [], []\n",
    "for epoch in range(training_epochs):\n",
    "    # go through trainng set\n",
    "    c = []\n",
    "    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n",
    "        c.append(train_ffn(trX[start:end], trY[start:end]))\n",
    "    ff_training_cost.append(np.mean(c, dtype='float64')) # training cost\n",
    "    ff_acc.append(np.mean(np.argmax(teY, axis=1) == test_ffn(teX))) # accuracy\n",
    "    print(ff_acc[epoch])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result plotting for question B2\n",
    "plot_training_error(ff_training_cost, '3/2')\n",
    "plot_test_accuracy(ff_acc, '3/2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
