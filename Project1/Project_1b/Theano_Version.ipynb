{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "np.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale and normalize input data\n",
    "def scale(X, X_min, X_max):\n",
    "    return (X - X_min)/(X_max - X_min)\n",
    " \n",
    "def normalize(X, X_mean, X_std):\n",
    "    return (X - X_mean)/X_std\n",
    "\n",
    "def shuffle_data (samples, labels):\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    #print  (samples.shape, labels.shape)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels\n",
    "\n",
    "# update parameters\n",
    "def sgd(cost, params, lr):\n",
    "    grads = T.grad(cost=cost, wrt=params)\n",
    "    updates = []\n",
    "    for p, g in zip(params, grads):\n",
    "        updates.append([p, p - g * lr])\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read and divide data into test and train sets\n",
    "cal_housing = np.loadtxt('cal_housing.data', delimiter=',')\n",
    "X_data, Y_data = cal_housing[:,:8], cal_housing[:,-1]\n",
    "Y_data = (np.asmatrix(Y_data)).transpose()\n",
    "\n",
    "X_data, Y_data = shuffle_data(X_data, Y_data)\n",
    "\n",
    "\n",
    "#separate train and test data\n",
    "m = 3*X_data.shape[0] // 10\n",
    "testX, testY = X_data[:m],Y_data[:m]\n",
    "trainX, trainY = X_data[m:], Y_data[m:]\n",
    "\n",
    "# scale and normalize data\n",
    "trainX_max, trainX_min =  np.max(trainX, axis=0), np.min(trainX, axis=0)\n",
    "testX_max, testX_min =  np.max(testX, axis=0), np.min(testX, axis=0)\n",
    "\n",
    "trainX = scale(trainX, trainX_min, trainX_max)\n",
    "testX = scale(testX, trainX_min, trainX_max)\n",
    "\n",
    "# trainX_mean, trainX_std = np.mean(trainX, axis=0), np.std(trainX, axis=0)\n",
    "# testX_mean, testX_std = np.mean(testX, axis=0), np.std(testX, axis=0)\n",
    "\n",
    "# trainX = normalize(trainX, trainX_mean, trainX_std)\n",
    "# testX = normalize(testX, testX_mean, testX_std)\n",
    "\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, plot_filename):\n",
    "    \n",
    "    floatX = theano.config.floatX\n",
    "\n",
    "    no_features = trainX.shape[1] \n",
    "    x = T.matrix('x') # data sample\n",
    "    d = T.matrix('d') # desired output\n",
    "    no_samples = T.scalar('no_samples')\n",
    "\n",
    "    # initialize weights and biases for hidden layer(s) and output layer\n",
    "    w_o = theano.shared(np.random.randn(no_hidden1)*.01, floatX ) \n",
    "    b_o = theano.shared(np.random.randn()*.01, floatX)\n",
    "    w_h1 = theano.shared(np.random.randn(no_features, no_hidden1)*.01, floatX )\n",
    "    b_h1 = theano.shared(np.random.randn(no_hidden1)*0.01, floatX)\n",
    "\n",
    "    # learning rate\n",
    "    alpha = theano.shared(learning_rate, floatX) \n",
    "\n",
    "\n",
    "    #Define mathematical expression:\n",
    "    h1_out = T.nnet.sigmoid(T.dot(x, w_h1) + b_h1)\n",
    "    y = T.dot(h1_out, w_o) + b_o\n",
    "\n",
    "    cost = T.abs_(T.mean(T.sqr(d - y)))\n",
    "    accuracy = T.mean(d - y)\n",
    "\n",
    "    #define gradients\n",
    "    dw_o, db_o, dw_h, db_h = T.grad(cost, [w_o, b_o, w_h1, b_h1])\n",
    "\n",
    "#     params = [w_o, b_o, w_h1, b_h1]\n",
    "#     updates = sgd(cost, params, alpha)\n",
    "    \n",
    "    \n",
    "    train = theano.function(\n",
    "            inputs = [x, d],\n",
    "            outputs = cost,\n",
    "            updates = [[w_o, w_o - alpha*dw_o],\n",
    "                   [b_o, b_o - alpha*db_o],\n",
    "                   [w_h1, w_h1 - alpha*dw_h],\n",
    "                   [b_h1, b_h1 - alpha*db_h]],\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "#     train = theano.function(\n",
    "#         inputs = [x, d],\n",
    "#         outputs = cost,\n",
    "#         updates = updates,\n",
    "#         allow_input_downcast=True\n",
    "#         )\n",
    "\n",
    "    test = theano.function(\n",
    "        inputs = [x, d],\n",
    "        outputs = [y, cost, accuracy],\n",
    "        allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "\n",
    "    train_cost = np.zeros(epochs)\n",
    "    test_cost = np.zeros(epochs)\n",
    "    test_accuracy = np.zeros(epochs)\n",
    "\n",
    "    min_error = 1e+15\n",
    "    best_iter = 0\n",
    "    best_w_o = np.zeros(no_hidden1)\n",
    "    best_w_h1 = np.zeros([no_features, no_hidden1])\n",
    "    best_b_o = 0\n",
    "    best_b_h1 = np.zeros(no_hidden1)\n",
    "\n",
    "    alpha.set_value(learning_rate)\n",
    "    print(alpha.get_value())\n",
    "\n",
    "\n",
    "    # Training\n",
    "    val_itr = 0\n",
    "    t = time.time()\n",
    "\n",
    "    val_accuracy = np.zeros(epochs)\n",
    "    train_cost = np.zeros(epochs)\n",
    "    test_cost = np.zeros(epochs)\n",
    "    test_accuracy = np.zeros(epochs)\n",
    "\n",
    "    for iter in range(epochs):\n",
    "        if iter % 100 == 0:\n",
    "            print(iter)\n",
    "\n",
    "        trainX, trainY = shuffle_data(trainX, trainY)\n",
    "        cost = 0.0\n",
    "        n = len(trainX)\n",
    "        for start, end in zip(range(0, n, batch_size), range(batch_size, n, batch_size)):\n",
    "            cost += train(trainX[start:end], np.transpose(trainY[start:end]))\n",
    "#         train_cost[iter] = train(trainX, np.transpose(trainY))\n",
    "        train_cost[iter] = cost/(n // batch_size)\n",
    "        pred, test_cost[iter], test_accuracy[iter] = test(testX, np.transpose(testY))\n",
    "\n",
    "        if test_cost[iter] < min_error:\n",
    "            best_iter = iter\n",
    "            min_error = test_cost[iter]\n",
    "            best_w_o = w_o.get_value()\n",
    "            best_w_h1 = w_h1.get_value()\n",
    "            best_b_o = b_o.get_value()\n",
    "            best_b_h1 = b_h1.get_value()\n",
    "\n",
    "    #set weights and biases to values at which performance was best\n",
    "    w_o.set_value(best_w_o)\n",
    "    b_o.set_value(best_b_o)\n",
    "    w_h1.set_value(best_w_h1)\n",
    "    b_h1.set_value(best_b_h1)\n",
    "\n",
    "    best_pred, best_cost, best_accuracy = test(testX, np.transpose(testY))\n",
    "\n",
    "    print('Minimum error: %.1f, Best accuracy %.1f, Number of Iterations: %d'%(best_cost, best_accuracy, best_iter))\n",
    "    \n",
    "    plot_train_error(plot_filename, train_cost, epochs)\n",
    "    plot_test_error(plot_filename, test_cost, epochs)\n",
    "    plot_test_accuracy(plot_filename, test_accuracy, epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, plot_filename):\n",
    "    \n",
    "    floatX = theano.config.floatX\n",
    "\n",
    "    no_features = trainX.shape[1] \n",
    "    x = T.matrix('x') # data sample\n",
    "    d = T.matrix('d') # desired output\n",
    "    no_samples = T.scalar('no_samples')\n",
    "\n",
    "    # initialize weights and biases for hidden layer(s) and output layer\n",
    "    w_o = theano.shared(np.random.randn(no_hidden1)*.01, floatX ) \n",
    "    b_o = theano.shared(np.random.randn()*.01, floatX)\n",
    "    w_h1 = theano.shared(np.random.randn(no_features, no_hidden1)*.01, floatX )\n",
    "    b_h1 = theano.shared(np.random.randn(no_hidden1)*0.01, floatX)\n",
    "\n",
    "    # learning rate\n",
    "    alpha = theano.shared(learning_rate, floatX) \n",
    "\n",
    "\n",
    "    #Define mathematical expression:\n",
    "    h1_out = T.nnet.sigmoid(T.dot(x, w_h1) + b_h1)\n",
    "    y = T.dot(h1_out, w_o) + b_o\n",
    "\n",
    "    cost = T.abs_(T.mean(T.sqr(d - y)))\n",
    "    accuracy = T.mean(d - y)\n",
    "\n",
    "    #define gradients\n",
    "\n",
    "    dw_o, db_o, dw_h, db_h = T.grad(cost, [w_o, b_o, w_h1, b_h1])\n",
    "\n",
    "    \n",
    "    \n",
    "    train = theano.function(\n",
    "            inputs = [x, d],\n",
    "            outputs = cost,\n",
    "            updates = [[w_o, w_o - alpha*dw_o],\n",
    "                   [b_o, b_o - alpha*db_o],\n",
    "                   [w_h1, w_h1 - alpha*dw_h],\n",
    "                   [b_h1, b_h1 - alpha*db_h]],\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "    test = theano.function(\n",
    "        inputs = [x, d],\n",
    "        outputs = [y, cost, accuracy],\n",
    "        allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    kf = KFold(n_splits=5)\n",
    "    val_itr = 0\n",
    "    t = time.time()\n",
    "    \n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    val_costs = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(trainX):\n",
    "\n",
    "        train_cost = np.zeros(epochs)\n",
    "        val_cost = np.zeros(epochs)\n",
    "        val_accuracy = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        train_cost = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        w_o.set_value(np.random.randn(no_hidden1)*.01)\n",
    "        b_o.set_value(np.random.randn()*.01)\n",
    "        w_h1.set_value(np.random.randn(no_features, no_hidden1)*.01)\n",
    "        b_h1.set_value(np.random.randn(no_hidden1)*.01)\n",
    "\n",
    "        min_error = 1e+15\n",
    "        best_iter = 0\n",
    "        best_w_o = np.zeros(no_hidden1)\n",
    "        best_w_h1 = np.zeros([no_features, no_hidden1])\n",
    "        best_b_o = 0\n",
    "        best_b_h1 = np.zeros(no_hidden1)\n",
    "\n",
    "        alpha.set_value(learning_rate)\n",
    "        print(alpha.get_value())\n",
    "\n",
    "        val_itr += 1\n",
    "        print(\"k fold validation: \" + str(val_itr))\n",
    "        print(\"TRAIN: \"+str(train_index) + \" VALID: \"+str(val_index))\n",
    "        \n",
    "        val_set_X = trainX[val_index]\n",
    "        val_set_Y = trainY[val_index]\n",
    "        train_set_X = trainX[train_index]\n",
    "        train_set_Y = trainY[train_index]\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            if iter % 100 == 0:\n",
    "                print(iter)\n",
    "\n",
    "            trainX, trainY = shuffle_data(trainX, trainY)\n",
    "#             cost = 0.0\n",
    "#             n = len(train_set_X)\n",
    "#             for start, end in zip(range(0, n, batch_size), range(batch_size, n, batch_size)):\n",
    "#                 cost += train(train_set_X[start:end], np.transpose(train_set_Y[start:end]))\n",
    "            \n",
    "#             train_cost[iter] = cost/(n // batch_size)\n",
    "            train_cost[iter] = train(trainX, np.transpose(trainY))\n",
    "            val_pred, val_cost[iter], val_accuracy[iter] = test(val_set_X, np.transpose(val_set_Y))\n",
    "            pred, test_cost[iter], test_accuracy[iter] = test(testX, np.transpose(testY))\n",
    "\n",
    "            if test_cost[iter] < min_error:\n",
    "                best_iter = iter\n",
    "                min_error = test_cost[iter]\n",
    "                best_w_o = w_o.get_value()\n",
    "                best_w_h1 = w_h1.get_value()\n",
    "                best_b_o = b_o.get_value()\n",
    "                best_b_h1 = b_h1.get_value()\n",
    "\n",
    "        #set weights and biases to values at which performance was best\n",
    "        w_o.set_value(best_w_o)\n",
    "        b_o.set_value(best_b_o)\n",
    "        w_h1.set_value(best_w_h1)\n",
    "        b_h1.set_value(best_b_h1)\n",
    "        \n",
    "        test_costs.append(test_cost)\n",
    "        train_costs.append(train_cost)\n",
    "        val_costs.append(val_cost)\n",
    "        \n",
    "\n",
    "        best_pred, best_cost, best_accuracy = test(testX, np.transpose(testY))\n",
    "\n",
    "        print('Minimum error: %.1f, Best accuracy %.1f, Number of Iterations: %d'%(best_cost, best_accuracy, best_iter))\n",
    "        \n",
    "\n",
    "        \n",
    "    plot_train_val_error(plot_filename, np.mean(train_costs, axis=0), np.mean(val_costs, axis=0), epochs)\n",
    "    plot_test_error(plot_filename, np.mean(test_costs, axis=0), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_4layers_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, no_hidden2, plot_filename):\n",
    "    \n",
    "    floatX = theano.config.floatX\n",
    "\n",
    "    no_features = trainX.shape[1] \n",
    "    x = T.matrix('x') # data sample\n",
    "    d = T.matrix('d') # desired output\n",
    "    no_samples = T.scalar('no_samples')\n",
    "\n",
    "    # initialize weights and biases for hidden layer(s) and output layer\n",
    "    w_o = theano.shared(np.random.randn(no_hidden2)*.01, floatX ) \n",
    "    b_o = theano.shared(np.random.randn()*.01, floatX)\n",
    "    w_h2 = theano.shared(np.random.randn(no_hidden1, no_hidden2)*.01, floatX )\n",
    "    b_h2 = theano.shared(np.random.randn(no_hidden2)*0.01, floatX)\n",
    "    w_h1 = theano.shared(np.random.randn(no_features, no_hidden1)*.01, floatX )\n",
    "    b_h1 = theano.shared(np.random.randn(no_hidden1)*0.01, floatX)\n",
    " \n",
    "\n",
    "    # learning rate\n",
    "    alpha = theano.shared(learning_rate, floatX) \n",
    "\n",
    "\n",
    "    #Define mathematical expression:\n",
    "    h1_out = T.nnet.sigmoid(T.dot(x, w_h1) + b_h1)\n",
    "    h2_out = T.nnet.sigmoid(T.dot(h1_out, w_h2) + b_h2)\n",
    "    y = T.dot(h2_out, w_o) + b_o\n",
    "\n",
    "    cost = T.abs_(T.mean(T.sqr(d - y)))\n",
    "    accuracy = T.mean(d - y)\n",
    "\n",
    "    #define gradients\n",
    "\n",
    "    dw_o, db_o, dw_h1, db_h1, dw_h2, db_h2 = T.grad(cost, [w_o, b_o, w_h1, b_h1, w_h2, b_h2])\n",
    "\n",
    "    \n",
    "    \n",
    "    train = theano.function(\n",
    "            inputs = [x, d],\n",
    "            outputs = cost,\n",
    "            updates = [[w_o, w_o - alpha*dw_o],\n",
    "                   [b_o, b_o - alpha*db_o],\n",
    "                   [w_h1, w_h1 - alpha*dw_h1],\n",
    "                   [b_h1, b_h1 - alpha*db_h1],\n",
    "                   [w_h2, w_h2 - alpha*dw_h2],\n",
    "                   [b_h2, b_h2 - alpha*db_h2]],\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "    test = theano.function(\n",
    "        inputs = [x, d],\n",
    "        outputs = [y, cost, accuracy],\n",
    "        allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    kf = KFold(n_splits=5)\n",
    "    val_itr = 0\n",
    "    t = time.time()\n",
    "    \n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    val_costs = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(trainX):\n",
    "\n",
    "        train_cost = np.zeros(epochs)\n",
    "        val_cost = np.zeros(epochs)\n",
    "        val_accuracy = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        train_cost = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        w_o.set_value(np.random.randn(no_hidden2)*.01)\n",
    "        b_o.set_value(np.random.randn()*.01)\n",
    "        w_h2.set_value(np.random.randn(no_hidden1, no_hidden2)*.01)\n",
    "        b_h2.set_value(np.random.randn(no_hidden2)*.01)\n",
    "        w_h1.set_value(np.random.randn(no_features, no_hidden1)*.01)\n",
    "        b_h1.set_value(np.random.randn(no_hidden1)*.01)\n",
    "\n",
    "\n",
    "        min_error = 1e+15\n",
    "        best_iter = 0\n",
    "        best_w_o = np.zeros(no_hidden1)\n",
    "        best_w_h1 = np.zeros([no_features, no_hidden1])\n",
    "        best_b_o = 0\n",
    "        best_b_h1 = np.zeros(no_hidden1)\n",
    "\n",
    "        alpha.set_value(learning_rate)\n",
    "        print(alpha.get_value())\n",
    "\n",
    "        val_itr += 1\n",
    "        print(\"k fold validation: \" + str(val_itr))\n",
    "        print(\"TRAIN: \"+str(train_index) + \" VALID: \"+str(val_index))\n",
    "        \n",
    "        val_set_X = trainX[val_index]\n",
    "        val_set_Y = trainY[val_index]\n",
    "        train_set_X = trainX[train_index]\n",
    "        train_set_Y = trainY[train_index]\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            if iter % 100 == 0:\n",
    "                print(iter)\n",
    "\n",
    "            trainX, trainY = shuffle_data(trainX, trainY)\n",
    "#             cost = 0.0\n",
    "#             n = len(train_set_X)\n",
    "#             for start, end in zip(range(0, n, batch_size), range(batch_size, n, batch_size)):\n",
    "#                 cost += train(train_set_X[start:end], np.transpose(train_set_Y[start:end]))\n",
    "            \n",
    "#             train_cost[iter] = cost/(n // batch_size)\n",
    "            train_cost[iter] = train(trainX, np.transpose(trainY))\n",
    "            val_pred, val_cost[iter], val_accuracy[iter] = test(val_set_X, np.transpose(val_set_Y))\n",
    "            pred, test_cost[iter], test_accuracy[iter] = test(testX, np.transpose(testY))\n",
    "\n",
    "            if test_cost[iter] < min_error:\n",
    "                best_iter = iter\n",
    "                min_error = test_cost[iter]\n",
    "                best_w_o = w_o.get_value()\n",
    "                best_w_h1 = w_h1.get_value()\n",
    "                best_b_o = b_o.get_value()\n",
    "                best_b_h1 = b_h1.get_value()\n",
    "\n",
    "        #set weights and biases to values at which performance was best\n",
    "        w_o.set_value(best_w_o)\n",
    "        b_o.set_value(best_b_o)\n",
    "        w_h1.set_value(best_w_h1)\n",
    "        b_h1.set_value(best_b_h1)\n",
    "        \n",
    "        test_costs.append(test_cost)\n",
    "        train_costs.append(train_cost)\n",
    "        val_costs.append(val_cost)\n",
    "        \n",
    "\n",
    "        best_pred, best_cost, best_accuracy = test(testX, np.transpose(testY))\n",
    "\n",
    "        print('Minimum error: %.1f, Best accuracy %.1f, Number of Iterations: %d'%(best_cost, best_accuracy, best_iter))\n",
    "        \n",
    "\n",
    "        \n",
    "    plot_train_val_error(plot_filename, np.mean(train_costs, axis=0), np.mean(val_costs, axis=0), epochs)\n",
    "    plot_test_error(plot_filename, np.mean(test_costs, axis=0), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_5layers_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, no_hidden2, no_hidden3, plot_filename):\n",
    "    \n",
    "    floatX = theano.config.floatX\n",
    "\n",
    "    no_features = trainX.shape[1] \n",
    "    x = T.matrix('x') # data sample\n",
    "    d = T.matrix('d') # desired output\n",
    "    no_samples = T.scalar('no_samples')\n",
    "\n",
    "    # initialize weights and biases for hidden layer(s) and output layer\n",
    "    w_o = theano.shared(np.random.randn(no_hidden3)*.01, floatX ) \n",
    "    b_o = theano.shared(np.random.randn()*.01, floatX)\n",
    "    w_h3 = theano.shared(np.random.randn(no_hidden2, no_hidden3)*.01, floatX )\n",
    "    b_h3 = theano.shared(np.random.randn(no_hidden3)*0.01, floatX)\n",
    "    w_h2 = theano.shared(np.random.randn(no_hidden1, no_hidden2)*.01, floatX )\n",
    "    b_h2 = theano.shared(np.random.randn(no_hidden2)*0.01, floatX)\n",
    "    w_h1 = theano.shared(np.random.randn(no_features, no_hidden1)*.01, floatX )\n",
    "    b_h1 = theano.shared(np.random.randn(no_hidden1)*0.01, floatX)\n",
    " \n",
    "\n",
    "    # learning rate\n",
    "    alpha = theano.shared(learning_rate, floatX) \n",
    "\n",
    "\n",
    "    #Define mathematical expression:\n",
    "    h1_out = T.nnet.sigmoid(T.dot(x, w_h1) + b_h1)\n",
    "    h2_out = T.nnet.sigmoid(T.dot(h1_out, w_h2) + b_h2)\n",
    "    h3_out = T.nnet.sigmoid(T.dot(h2_out, w_h3) + b_h3)\n",
    "    y = T.dot(h3_out, w_o) + b_o\n",
    "\n",
    "    cost = T.abs_(T.mean(T.sqr(d - y)))\n",
    "    accuracy = T.mean(d - y)\n",
    "\n",
    "    #define gradients\n",
    "\n",
    "    dw_o, db_o, dw_h1, db_h1, dw_h2, db_h2, dw_h3, db_h3 = T.grad(cost, [w_o, b_o, w_h1, b_h1, w_h2, b_h2, w_h3, b_h3])\n",
    "\n",
    "    \n",
    "    \n",
    "    train = theano.function(\n",
    "            inputs = [x, d],\n",
    "            outputs = cost,\n",
    "            updates = [[w_o, w_o - alpha*dw_o],\n",
    "                   [b_o, b_o - alpha*db_o],\n",
    "                   [w_h1, w_h1 - alpha*dw_h1],\n",
    "                   [b_h1, b_h1 - alpha*db_h1],\n",
    "                   [w_h2, w_h2 - alpha*dw_h2],\n",
    "                   [b_h2, b_h2 - alpha*db_h2],\n",
    "                   [w_h3, w_h3 - alpha*dw_h3],\n",
    "                   [b_h3, b_h3 - alpha*db_h3]],\n",
    "            allow_input_downcast=True\n",
    "            )\n",
    "\n",
    "    test = theano.function(\n",
    "        inputs = [x, d],\n",
    "        outputs = [y, cost, accuracy],\n",
    "        allow_input_downcast=True\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Training\n",
    "    kf = KFold(n_splits=5)\n",
    "    val_itr = 0\n",
    "    t = time.time()\n",
    "    \n",
    "    train_costs = []\n",
    "    test_costs = []\n",
    "    val_costs = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(trainX):\n",
    "\n",
    "        train_cost = np.zeros(epochs)\n",
    "        val_cost = np.zeros(epochs)\n",
    "        val_accuracy = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        train_cost = np.zeros(epochs)\n",
    "        test_cost = np.zeros(epochs)\n",
    "        test_accuracy = np.zeros(epochs)\n",
    "        \n",
    "        w_o.set_value(np.random.randn(no_hidden3)*.01)\n",
    "        b_o.set_value(np.random.randn()*.01)\n",
    "        w_h3.set_value(np.random.randn(no_hidden2, no_hidden3)*.01)\n",
    "        b_h3.set_value(np.random.randn(no_hidden3)*.01)\n",
    "        w_h2.set_value(np.random.randn(no_hidden1, no_hidden2)*.01)\n",
    "        b_h2.set_value(np.random.randn(no_hidden2)*.01)\n",
    "        w_h1.set_value(np.random.randn(no_features, no_hidden1)*.01)\n",
    "        b_h1.set_value(np.random.randn(no_hidden1)*.01)\n",
    "\n",
    "\n",
    "        min_error = 1e+15\n",
    "        best_iter = 0\n",
    "        best_w_o = np.zeros(no_hidden1)\n",
    "        best_w_h1 = np.zeros([no_features, no_hidden1])\n",
    "        best_b_o = 0\n",
    "        best_b_h1 = np.zeros(no_hidden1)\n",
    "\n",
    "        alpha.set_value(learning_rate)\n",
    "        print(alpha.get_value())\n",
    "\n",
    "        val_itr += 1\n",
    "        print(\"k fold validation: \" + str(val_itr))\n",
    "        print(\"TRAIN: \"+str(train_index) + \" VALID: \"+str(val_index))\n",
    "        \n",
    "        val_set_X = trainX[val_index]\n",
    "        val_set_Y = trainY[val_index]\n",
    "        train_set_X = trainX[train_index]\n",
    "        train_set_Y = trainY[train_index]\n",
    "\n",
    "        for iter in range(epochs):\n",
    "            if iter % 100 == 0:\n",
    "                print(iter)\n",
    "\n",
    "            trainX, trainY = shuffle_data(trainX, trainY)\n",
    "#             cost = 0.0\n",
    "#             n = len(train_set_X)\n",
    "#             for start, end in zip(range(0, n, batch_size), range(batch_size, n, batch_size)):\n",
    "#                 cost += train(train_set_X[start:end], np.transpose(train_set_Y[start:end]))\n",
    "            \n",
    "#             train_cost[iter] = cost/(n // batch_size)\n",
    "            train_cost[iter] = train(trainX, np.transpose(trainY))\n",
    "            val_pred, val_cost[iter], val_accuracy[iter] = test(val_set_X, np.transpose(val_set_Y))\n",
    "            pred, test_cost[iter], test_accuracy[iter] = test(testX, np.transpose(testY))\n",
    "\n",
    "            if test_cost[iter] < min_error:\n",
    "                best_iter = iter\n",
    "                min_error = test_cost[iter]\n",
    "                best_w_o = w_o.get_value()\n",
    "                best_w_h1 = w_h1.get_value()\n",
    "                best_b_o = b_o.get_value()\n",
    "                best_b_h1 = b_h1.get_value()\n",
    "\n",
    "        #set weights and biases to values at which performance was best\n",
    "        w_o.set_value(best_w_o)\n",
    "        b_o.set_value(best_b_o)\n",
    "        w_h1.set_value(best_w_h1)\n",
    "        b_h1.set_value(best_b_h1)\n",
    "        \n",
    "        test_costs.append(test_cost)\n",
    "        train_costs.append(train_cost)\n",
    "        val_costs.append(val_cost)\n",
    "        \n",
    "\n",
    "        best_pred, best_cost, best_accuracy = test(testX, np.transpose(testY))\n",
    "\n",
    "        print('Minimum error: %.1f, Best accuracy %.1f, Number of Iterations: %d'%(best_cost, best_accuracy, best_iter))\n",
    "        \n",
    "\n",
    "        \n",
    "    plot_train_val_error(plot_filename, np.mean(train_costs, axis=0), np.mean(val_costs, axis=0), epochs)\n",
    "    plot_test_error(plot_filename, np.mean(test_costs, axis=0), epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annot_max(x,y, ax=None):\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = y.max()\n",
    "    text= \"MAX Point x={:.3f}, y={:.3f}\".format(xmax, ymax)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=60\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.94,0.1), **kw)\n",
    "\n",
    "def annot_min(x,y, ax=None):\n",
    "    xmin = x[np.argmin(y)]\n",
    "    ymin = y.min()\n",
    "    text= \"MIN Point x={:.3f}, y={:.3f}\".format(xmin, ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"arc3\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"bottom\")\n",
    "    ax.annotate(text, xy=(xmin, ymin), xytext=(0.94,0.9), **kw)\n",
    "\n",
    "def annot_min2(x,y, ax=None):\n",
    "    xmin = x[np.argmin(y)]\n",
    "    ymin = y.min()\n",
    "    text= \"MIN Point x={:.3f}, y={:.3f}\".format(xmin, ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=120\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"bottom\")\n",
    "    ax.annotate(text, xy=(xmin, ymin), xytext=(0.94,0.7), **kw)\n",
    "    \n",
    "    \n",
    "def plot_train_error(filename_prefix, train_cost, epochs=1000):\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), train_cost, label='train error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Training Errors')\n",
    "#     plt.legend()\n",
    "    annot_min(range(epochs), train_cost)\n",
    "    plt.savefig(filename_prefix + '_train_error.png')\n",
    "    \n",
    "def plot_test_error(filename_prefix, test_cost, epochs=1000):\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), test_cost, label='train error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Test Errors')\n",
    "#     plt.legend()\n",
    "    annot_min(range(epochs), test_cost)\n",
    "    plt.savefig(filename_prefix + '_test_error.png')\n",
    "    \n",
    "def plot_test_accuracy(filename_prefix, test_accuracy, epochs=1000):\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), test_accuracy, label='test accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Test Accuracy')\n",
    "#     plt.legend()\n",
    "    annot_min(range(epochs), test_accuracy)\n",
    "    plt.savefig(filename_prefix + '_test_accuracy.png') \n",
    "\n",
    "def plot_train_val_error(filename_prefix, train_cost, val_cost, epochs=1000):\n",
    "    plt.figure()\n",
    "    plt.plot(range(epochs), train_cost, label='train error')\n",
    "    plt.plot(range(epochs), val_cost, label='validation error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Training and Validation Errors')\n",
    "#     annot_min(range(epochs),train_cost)\n",
    "#     annot_min2(range(epochs),val_cost)\n",
    "    plt.legend()\n",
    "    plt.savefig(filename_prefix + '_train_val_error.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Question 1\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "no_hidden1 = 30 #num of neurons in hidden layer 1\n",
    "learning_rate = 1e-4\n",
    "\n",
    "train_network(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, './theano_graph/1/Question1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 2\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "no_hidden1 = 30 #num of neurons in hidden layer 1\n",
    "learning_rates = [1e-3, 0.5e-3, 1e-4, 0.5e-4, 1e-5]\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    print(\"Running Learning Rate: \"+ str(learning_rate))\n",
    "    train_network_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, './theano_graph/2/learning_rate_'+str(learning_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 3\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "nos_hidden1 = [20, 30, 40, 50 ,60] #num of neurons in hidden layer 1\n",
    "learning_rate = 1e-3\n",
    "\n",
    "for no_hidden1 in nos_hidden1:\n",
    "    print(\"Running Num of Hidden 1: \"+ str(no_hidden1))\n",
    "    train_network_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, './theano_graph/3/no_hidden1_'+str(no_hidden1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 4\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "no_hidden1 = 60 #num of neurons in hidden layer 1\n",
    "no_hidden2 = 20\n",
    "no_hidden3 = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "train_4layers_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, no_hidden2, './theano_graph/4/layers_4')\n",
    "train_5layers_validation(trainX, trainY, testX, testY, learning_rate, epochs, batch_size, no_hidden1, no_hidden2, no_hidden3, './theano_graph/4/layers_5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
