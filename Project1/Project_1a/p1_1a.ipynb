{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from keras import optimizers, callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_pic_folder = \"./graph/\"\n",
    "#  create folder if not exists\n",
    "if not os.path.exists(plot_pic_folder):\n",
    "    os.makedirs(plot_pic_folder)\n",
    "num_of_question = 5\n",
    "for i in range(1,num_of_question+1):\n",
    "    if not os.path.exists(plot_pic_folder + str(i) + '/'):\n",
    "        os.makedirs(plot_pic_folder + str(i) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchTime(callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.logs=[]\n",
    "        self.batch_time = []\n",
    "        self.start_time = []\n",
    "        self.cur_start_time = 0\n",
    "        \n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.cur_start_time = time.time()\n",
    "        self.start_time.append(self.cur_start_time)\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.batch_time.append(time.time() - self.cur_start_time)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data\n",
    "def scale(X, X_min, X_max):\n",
    "    # min-max normalization\n",
    "    return (X - X_min)/(X_max-np.min(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data (samples, labels):\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    #print  (samples.shape, labels.shape)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Point out the maximum and minimum point\n",
    "\n",
    "def annot_max(x,y, ax=None):\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = y.max()\n",
    "    text= \"MAX Point x={:.3f}, y={:.3f}\".format(xmax, ymax)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=60\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.94,0.96), **kw)\n",
    "\n",
    "def annot_min(x,y, ax=None):\n",
    "    xmin = x[np.argmin(y)]\n",
    "    ymin = y.min()\n",
    "    text= \"MIN Point x={:.3f}, y={:.3f}\".format(xmin, ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=120\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"bottom\")\n",
    "    ax.annotate(text, xy=(xmin, ymin), xytext=(0.94,0.96), **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4435, 36) (4435, 6)\n",
      "(2000, 36) (2000, 6)\n"
     ]
    }
   ],
   "source": [
    "#read train data\n",
    "train_input = np.loadtxt('./data/sat_train.txt',delimiter=' ')\n",
    "trainX, train_Y = train_input[:,:36], train_input[:,-1].astype(int)\n",
    "trainX_min, trainX_max = np.min(trainX, axis=0), np.max(trainX, axis=0)\n",
    "trainX = scale(trainX, trainX_min, trainX_max)\n",
    "\n",
    "train_Y[train_Y == 7] = 6\n",
    "trainY = np.zeros((train_Y.shape[0], 6))\n",
    "trainY[np.arange(train_Y.shape[0]), train_Y-1] = 1\n",
    "\n",
    "\n",
    "#read test data\n",
    "test_input = np.loadtxt('./data/sat_test.txt',delimiter=' ')\n",
    "testX, test_Y = test_input[:,:36], test_input[:,-1].astype(int)\n",
    "\n",
    "# testX_min, testX_max = np.min(testX, axis=0), np.max(testX, axis=0)\n",
    "testX = scale(testX, trainX_min, trainX_max)\n",
    "\n",
    "test_Y[test_Y == 7] = 6\n",
    "testY = np.zeros((test_Y.shape[0], 6))\n",
    "testY[np.arange(test_Y.shape[0]), test_Y-1] = 1\n",
    "\n",
    "del test_Y, train_Y, trainX_min, trainX_max\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/1000\n",
      "2s - loss: 1.4141 - acc: 0.4607 - val_loss: 1.1389 - val_acc: 0.6980\n",
      "Epoch 2/1000\n",
      "1s - loss: 0.9164 - acc: 0.7238 - val_loss: 0.8028 - val_acc: 0.7410\n",
      "Epoch 3/1000\n",
      "1s - loss: 0.6839 - acc: 0.7926 - val_loss: 0.6408 - val_acc: 0.7930\n",
      "Epoch 4/1000\n",
      "2s - loss: 0.5683 - acc: 0.8171 - val_loss: 0.5627 - val_acc: 0.8060\n",
      "Epoch 5/1000\n",
      "1s - loss: 0.5047 - acc: 0.8232 - val_loss: 0.5067 - val_acc: 0.8060\n",
      "Epoch 6/1000\n",
      "1s - loss: 0.4700 - acc: 0.8262 - val_loss: 0.5098 - val_acc: 0.8075\n",
      "Epoch 7/1000\n",
      "2s - loss: 0.4484 - acc: 0.8327 - val_loss: 0.4607 - val_acc: 0.8165\n",
      "Epoch 8/1000\n",
      "2s - loss: 0.4323 - acc: 0.8363 - val_loss: 0.4456 - val_acc: 0.8120\n",
      "Epoch 9/1000\n",
      "2s - loss: 0.4212 - acc: 0.8386 - val_loss: 0.4546 - val_acc: 0.8270\n",
      "Epoch 10/1000\n",
      "2s - loss: 0.4133 - acc: 0.8408 - val_loss: 0.4288 - val_acc: 0.8255\n",
      "Epoch 11/1000\n",
      "2s - loss: 0.4064 - acc: 0.8370 - val_loss: 0.4207 - val_acc: 0.8305\n",
      "Epoch 12/1000\n",
      "2s - loss: 0.4009 - acc: 0.8392 - val_loss: 0.4178 - val_acc: 0.8285\n",
      "Epoch 13/1000\n",
      "2s - loss: 0.3955 - acc: 0.8435 - val_loss: 0.4271 - val_acc: 0.8255\n",
      "Epoch 14/1000\n",
      "1s - loss: 0.3921 - acc: 0.8435 - val_loss: 0.4148 - val_acc: 0.8225\n",
      "Epoch 15/1000\n",
      "1s - loss: 0.3896 - acc: 0.8437 - val_loss: 0.4099 - val_acc: 0.8300\n",
      "Epoch 16/1000\n",
      "1s - loss: 0.3880 - acc: 0.8446 - val_loss: 0.4005 - val_acc: 0.8305\n",
      "Epoch 17/1000\n",
      "1s - loss: 0.3841 - acc: 0.8431 - val_loss: 0.4155 - val_acc: 0.8180\n",
      "Epoch 18/1000\n",
      "1s - loss: 0.3814 - acc: 0.8437 - val_loss: 0.4152 - val_acc: 0.8155\n",
      "Epoch 19/1000\n",
      "1s - loss: 0.3781 - acc: 0.8467 - val_loss: 0.4151 - val_acc: 0.8220\n",
      "Epoch 20/1000\n",
      "1s - loss: 0.3761 - acc: 0.8489 - val_loss: 0.4118 - val_acc: 0.8210\n",
      "Epoch 21/1000\n",
      "1s - loss: 0.3745 - acc: 0.8492 - val_loss: 0.3895 - val_acc: 0.8330\n",
      "Epoch 22/1000\n",
      "1s - loss: 0.3723 - acc: 0.8489 - val_loss: 0.3837 - val_acc: 0.8360\n",
      "Epoch 23/1000\n",
      "1s - loss: 0.3726 - acc: 0.8501 - val_loss: 0.3965 - val_acc: 0.8315\n",
      "Epoch 24/1000\n",
      "1s - loss: 0.3678 - acc: 0.8543 - val_loss: 0.3896 - val_acc: 0.8330\n",
      "Epoch 25/1000\n",
      "1s - loss: 0.3667 - acc: 0.8548 - val_loss: 0.3793 - val_acc: 0.8365\n",
      "Epoch 26/1000\n",
      "2s - loss: 0.3651 - acc: 0.8521 - val_loss: 0.3793 - val_acc: 0.8325\n",
      "Epoch 27/1000\n",
      "2s - loss: 0.3638 - acc: 0.8532 - val_loss: 0.3818 - val_acc: 0.8355\n",
      "Epoch 28/1000\n",
      "2s - loss: 0.3600 - acc: 0.8541 - val_loss: 0.3779 - val_acc: 0.8365\n",
      "Epoch 29/1000\n",
      "2s - loss: 0.3599 - acc: 0.8591 - val_loss: 0.3766 - val_acc: 0.8390\n",
      "Epoch 30/1000\n",
      "2s - loss: 0.3578 - acc: 0.8579 - val_loss: 0.4101 - val_acc: 0.8260\n",
      "Epoch 31/1000\n",
      "2s - loss: 0.3592 - acc: 0.8541 - val_loss: 0.3693 - val_acc: 0.8445\n",
      "Epoch 32/1000\n",
      "2s - loss: 0.3551 - acc: 0.8613 - val_loss: 0.3746 - val_acc: 0.8385\n",
      "Epoch 33/1000\n",
      "2s - loss: 0.3532 - acc: 0.8582 - val_loss: 0.3676 - val_acc: 0.8390\n",
      "Epoch 34/1000\n",
      "2s - loss: 0.3534 - acc: 0.8631 - val_loss: 0.3673 - val_acc: 0.8470\n",
      "Epoch 35/1000\n",
      "1s - loss: 0.3497 - acc: 0.8582 - val_loss: 0.3784 - val_acc: 0.8420\n",
      "Epoch 36/1000\n",
      "2s - loss: 0.3508 - acc: 0.8616 - val_loss: 0.3624 - val_acc: 0.8450\n",
      "Epoch 37/1000\n",
      "1s - loss: 0.3494 - acc: 0.8625 - val_loss: 0.3687 - val_acc: 0.8415\n",
      "Epoch 38/1000\n",
      "1s - loss: 0.3501 - acc: 0.8618 - val_loss: 0.3603 - val_acc: 0.8490\n",
      "Epoch 39/1000\n",
      "1s - loss: 0.3451 - acc: 0.8667 - val_loss: 0.3826 - val_acc: 0.8410\n",
      "Epoch 40/1000\n",
      "1s - loss: 0.3463 - acc: 0.8638 - val_loss: 0.3634 - val_acc: 0.8420\n",
      "Epoch 41/1000\n",
      "2s - loss: 0.3469 - acc: 0.8638 - val_loss: 0.3611 - val_acc: 0.8460\n",
      "Epoch 42/1000\n",
      "2s - loss: 0.3436 - acc: 0.8622 - val_loss: 0.3613 - val_acc: 0.8470\n",
      "Epoch 43/1000\n",
      "2s - loss: 0.3438 - acc: 0.8620 - val_loss: 0.3554 - val_acc: 0.8485\n",
      "Epoch 44/1000\n",
      "1s - loss: 0.3414 - acc: 0.8665 - val_loss: 0.3731 - val_acc: 0.8385\n",
      "Epoch 45/1000\n",
      "1s - loss: 0.3411 - acc: 0.8613 - val_loss: 0.3699 - val_acc: 0.8485\n",
      "Epoch 46/1000\n",
      "1s - loss: 0.3415 - acc: 0.8616 - val_loss: 0.3603 - val_acc: 0.8510\n",
      "Epoch 47/1000\n",
      "2s - loss: 0.3403 - acc: 0.8622 - val_loss: 0.3614 - val_acc: 0.8485\n",
      "Epoch 48/1000\n",
      "1s - loss: 0.3370 - acc: 0.8645 - val_loss: 0.3545 - val_acc: 0.8495\n",
      "Epoch 49/1000\n",
      "2s - loss: 0.3367 - acc: 0.8710 - val_loss: 0.3561 - val_acc: 0.8460\n",
      "Epoch 50/1000\n",
      "2s - loss: 0.3359 - acc: 0.8643 - val_loss: 0.3664 - val_acc: 0.8425\n",
      "Epoch 51/1000\n",
      "2s - loss: 0.3346 - acc: 0.8674 - val_loss: 0.3514 - val_acc: 0.8530\n",
      "Epoch 52/1000\n",
      "2s - loss: 0.3333 - acc: 0.8656 - val_loss: 0.3657 - val_acc: 0.8415\n",
      "Epoch 53/1000\n",
      "2s - loss: 0.3337 - acc: 0.8634 - val_loss: 0.3705 - val_acc: 0.8455\n",
      "Epoch 54/1000\n",
      "2s - loss: 0.3330 - acc: 0.8665 - val_loss: 0.3531 - val_acc: 0.8475\n",
      "Epoch 55/1000\n",
      "2s - loss: 0.3334 - acc: 0.8685 - val_loss: 0.3486 - val_acc: 0.8540\n",
      "Epoch 56/1000\n",
      "2s - loss: 0.3297 - acc: 0.8663 - val_loss: 0.3491 - val_acc: 0.8500\n",
      "Epoch 57/1000\n",
      "2s - loss: 0.3308 - acc: 0.8665 - val_loss: 0.3560 - val_acc: 0.8470\n",
      "Epoch 58/1000\n",
      "2s - loss: 0.3276 - acc: 0.8690 - val_loss: 0.3470 - val_acc: 0.8545\n",
      "Epoch 59/1000\n",
      "2s - loss: 0.3288 - acc: 0.8703 - val_loss: 0.3898 - val_acc: 0.8240\n",
      "Epoch 60/1000\n",
      "2s - loss: 0.3289 - acc: 0.8690 - val_loss: 0.3472 - val_acc: 0.8520\n",
      "Epoch 61/1000\n",
      "1s - loss: 0.3248 - acc: 0.8674 - val_loss: 0.3487 - val_acc: 0.8535\n",
      "Epoch 62/1000\n",
      "2s - loss: 0.3272 - acc: 0.8674 - val_loss: 0.3513 - val_acc: 0.8520\n",
      "Epoch 63/1000\n",
      "2s - loss: 0.3263 - acc: 0.8694 - val_loss: 0.3527 - val_acc: 0.8470\n",
      "Epoch 64/1000\n",
      "1s - loss: 0.3226 - acc: 0.8722 - val_loss: 0.3587 - val_acc: 0.8400\n",
      "Epoch 65/1000\n",
      "1s - loss: 0.3247 - acc: 0.8706 - val_loss: 0.3459 - val_acc: 0.8575\n",
      "Epoch 66/1000\n",
      "1s - loss: 0.3228 - acc: 0.8692 - val_loss: 0.3513 - val_acc: 0.8505\n",
      "Epoch 67/1000\n",
      "1s - loss: 0.3249 - acc: 0.8726 - val_loss: 0.3450 - val_acc: 0.8560\n",
      "Epoch 68/1000\n",
      "2s - loss: 0.3220 - acc: 0.8715 - val_loss: 0.3423 - val_acc: 0.8660\n",
      "Epoch 69/1000\n",
      "2s - loss: 0.3227 - acc: 0.8753 - val_loss: 0.3406 - val_acc: 0.8545\n",
      "Epoch 70/1000\n",
      "2s - loss: 0.3215 - acc: 0.8742 - val_loss: 0.3409 - val_acc: 0.8525\n",
      "Epoch 71/1000\n",
      "2s - loss: 0.3186 - acc: 0.8749 - val_loss: 0.3457 - val_acc: 0.8465\n",
      "Epoch 72/1000\n",
      "2s - loss: 0.3198 - acc: 0.8731 - val_loss: 0.3470 - val_acc: 0.8605\n",
      "Epoch 73/1000\n",
      "2s - loss: 0.3180 - acc: 0.8755 - val_loss: 0.3390 - val_acc: 0.8565\n",
      "Epoch 74/1000\n",
      "1s - loss: 0.3149 - acc: 0.8785 - val_loss: 0.3446 - val_acc: 0.8650\n",
      "Epoch 75/1000\n",
      "1s - loss: 0.3177 - acc: 0.8749 - val_loss: 0.3741 - val_acc: 0.8280\n",
      "Epoch 76/1000\n",
      "1s - loss: 0.3189 - acc: 0.8771 - val_loss: 0.3382 - val_acc: 0.8610\n",
      "Epoch 77/1000\n",
      "1s - loss: 0.3168 - acc: 0.8755 - val_loss: 0.3794 - val_acc: 0.8350\n",
      "Epoch 78/1000\n",
      "2s - loss: 0.3160 - acc: 0.8746 - val_loss: 0.3358 - val_acc: 0.8560\n",
      "Epoch 79/1000\n",
      "1s - loss: 0.3145 - acc: 0.8735 - val_loss: 0.3434 - val_acc: 0.8665\n",
      "Epoch 80/1000\n",
      "2s - loss: 0.3150 - acc: 0.8771 - val_loss: 0.3381 - val_acc: 0.8575\n",
      "Epoch 81/1000\n",
      "1s - loss: 0.3120 - acc: 0.8767 - val_loss: 0.3400 - val_acc: 0.8740\n",
      "Epoch 82/1000\n",
      "2s - loss: 0.3134 - acc: 0.8751 - val_loss: 0.3405 - val_acc: 0.8535\n",
      "Epoch 83/1000\n",
      "2s - loss: 0.3118 - acc: 0.8740 - val_loss: 0.3521 - val_acc: 0.8445\n",
      "Epoch 84/1000\n",
      "2s - loss: 0.3134 - acc: 0.8769 - val_loss: 0.3385 - val_acc: 0.8670\n",
      "Epoch 85/1000\n",
      "2s - loss: 0.3103 - acc: 0.8780 - val_loss: 0.3573 - val_acc: 0.8415\n",
      "Epoch 86/1000\n",
      "2s - loss: 0.3125 - acc: 0.8760 - val_loss: 0.3427 - val_acc: 0.8555\n",
      "Epoch 87/1000\n",
      "2s - loss: 0.3103 - acc: 0.8760 - val_loss: 0.3374 - val_acc: 0.8585\n",
      "Epoch 88/1000\n",
      "2s - loss: 0.3105 - acc: 0.8805 - val_loss: 0.3402 - val_acc: 0.8575\n",
      "Epoch 89/1000\n",
      "2s - loss: 0.3114 - acc: 0.8769 - val_loss: 0.3343 - val_acc: 0.8740\n",
      "Epoch 90/1000\n",
      "3s - loss: 0.3095 - acc: 0.8791 - val_loss: 0.3311 - val_acc: 0.8590\n",
      "Epoch 91/1000\n",
      "2s - loss: 0.3076 - acc: 0.8755 - val_loss: 0.3328 - val_acc: 0.8575\n",
      "Epoch 92/1000\n",
      "2s - loss: 0.3080 - acc: 0.8787 - val_loss: 0.3342 - val_acc: 0.8700\n",
      "Epoch 93/1000\n",
      "1s - loss: 0.3098 - acc: 0.8823 - val_loss: 0.3377 - val_acc: 0.8595\n",
      "Epoch 94/1000\n",
      "1s - loss: 0.3079 - acc: 0.8798 - val_loss: 0.3374 - val_acc: 0.8545\n",
      "Epoch 95/1000\n",
      "1s - loss: 0.3076 - acc: 0.8791 - val_loss: 0.3361 - val_acc: 0.8680\n",
      "Epoch 96/1000\n",
      "1s - loss: 0.3058 - acc: 0.8782 - val_loss: 0.3352 - val_acc: 0.8570\n",
      "Epoch 97/1000\n",
      "1s - loss: 0.3065 - acc: 0.8794 - val_loss: 0.3322 - val_acc: 0.8620\n",
      "Epoch 98/1000\n",
      "1s - loss: 0.3038 - acc: 0.8809 - val_loss: 0.3315 - val_acc: 0.8615\n",
      "Epoch 99/1000\n",
      "1s - loss: 0.3049 - acc: 0.8800 - val_loss: 0.3284 - val_acc: 0.8600\n",
      "Epoch 100/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.3050 - acc: 0.8807 - val_loss: 0.3375 - val_acc: 0.8640\n",
      "Epoch 101/1000\n",
      "2s - loss: 0.3053 - acc: 0.8796 - val_loss: 0.3397 - val_acc: 0.8570\n",
      "Epoch 102/1000\n",
      "1s - loss: 0.3065 - acc: 0.8794 - val_loss: 0.3301 - val_acc: 0.8605\n",
      "Epoch 103/1000\n",
      "1s - loss: 0.3028 - acc: 0.8780 - val_loss: 0.3309 - val_acc: 0.8590\n",
      "Epoch 104/1000\n",
      "1s - loss: 0.3036 - acc: 0.8809 - val_loss: 0.3461 - val_acc: 0.8530\n",
      "Epoch 105/1000\n",
      "1s - loss: 0.3018 - acc: 0.8791 - val_loss: 0.3347 - val_acc: 0.8670\n",
      "Epoch 106/1000\n",
      "1s - loss: 0.3025 - acc: 0.8796 - val_loss: 0.3317 - val_acc: 0.8630\n",
      "Epoch 107/1000\n",
      "1s - loss: 0.3016 - acc: 0.8809 - val_loss: 0.3270 - val_acc: 0.8730\n",
      "Epoch 108/1000\n",
      "1s - loss: 0.3022 - acc: 0.8814 - val_loss: 0.3456 - val_acc: 0.8580\n",
      "Epoch 109/1000\n",
      "1s - loss: 0.2986 - acc: 0.8852 - val_loss: 0.3273 - val_acc: 0.8695\n",
      "Epoch 110/1000\n",
      "2s - loss: 0.2986 - acc: 0.8832 - val_loss: 0.3343 - val_acc: 0.8730\n",
      "Epoch 111/1000\n",
      "2s - loss: 0.3006 - acc: 0.8852 - val_loss: 0.3363 - val_acc: 0.8565\n",
      "Epoch 112/1000\n",
      "2s - loss: 0.3017 - acc: 0.8823 - val_loss: 0.3508 - val_acc: 0.8540\n",
      "Epoch 113/1000\n",
      "2s - loss: 0.2993 - acc: 0.8780 - val_loss: 0.3297 - val_acc: 0.8605\n",
      "Epoch 114/1000\n",
      "2s - loss: 0.2971 - acc: 0.8834 - val_loss: 0.3254 - val_acc: 0.8645\n",
      "Epoch 115/1000\n",
      "2s - loss: 0.2972 - acc: 0.8798 - val_loss: 0.3335 - val_acc: 0.8570\n",
      "Epoch 116/1000\n",
      "2s - loss: 0.2988 - acc: 0.8825 - val_loss: 0.3376 - val_acc: 0.8545\n",
      "Epoch 117/1000\n",
      "1s - loss: 0.2981 - acc: 0.8812 - val_loss: 0.3359 - val_acc: 0.8535\n",
      "Epoch 118/1000\n",
      "1s - loss: 0.2986 - acc: 0.8832 - val_loss: 0.3453 - val_acc: 0.8460\n",
      "Epoch 119/1000\n",
      "1s - loss: 0.2967 - acc: 0.8843 - val_loss: 0.3282 - val_acc: 0.8695\n",
      "Epoch 120/1000\n",
      "1s - loss: 0.2983 - acc: 0.8852 - val_loss: 0.3257 - val_acc: 0.8655\n",
      "Epoch 121/1000\n",
      "1s - loss: 0.2980 - acc: 0.8828 - val_loss: 0.3264 - val_acc: 0.8665\n",
      "Epoch 122/1000\n",
      "2s - loss: 0.2957 - acc: 0.8870 - val_loss: 0.3308 - val_acc: 0.8575\n",
      "Epoch 123/1000\n",
      "2s - loss: 0.2946 - acc: 0.8828 - val_loss: 0.3260 - val_acc: 0.8740\n",
      "Epoch 124/1000\n",
      "3s - loss: 0.2938 - acc: 0.8861 - val_loss: 0.3581 - val_acc: 0.8385\n",
      "Epoch 125/1000\n",
      "2s - loss: 0.2953 - acc: 0.8843 - val_loss: 0.3263 - val_acc: 0.8680\n",
      "Epoch 126/1000\n",
      "2s - loss: 0.2941 - acc: 0.8870 - val_loss: 0.3345 - val_acc: 0.8640\n",
      "Epoch 127/1000\n",
      "2s - loss: 0.2929 - acc: 0.8859 - val_loss: 0.3325 - val_acc: 0.8525\n",
      "Epoch 128/1000\n",
      "1s - loss: 0.2949 - acc: 0.8864 - val_loss: 0.3266 - val_acc: 0.8645\n",
      "Epoch 129/1000\n",
      "2s - loss: 0.2937 - acc: 0.8857 - val_loss: 0.3375 - val_acc: 0.8625\n",
      "Epoch 130/1000\n",
      "2s - loss: 0.2937 - acc: 0.8875 - val_loss: 0.3263 - val_acc: 0.8730\n",
      "Epoch 131/1000\n",
      "2s - loss: 0.2935 - acc: 0.8846 - val_loss: 0.3241 - val_acc: 0.8705\n",
      "Epoch 132/1000\n",
      "2s - loss: 0.2932 - acc: 0.8877 - val_loss: 0.3276 - val_acc: 0.8630\n",
      "Epoch 133/1000\n",
      "2s - loss: 0.2929 - acc: 0.8875 - val_loss: 0.3347 - val_acc: 0.8670\n",
      "Epoch 134/1000\n",
      "2s - loss: 0.2913 - acc: 0.8839 - val_loss: 0.3257 - val_acc: 0.8715\n",
      "Epoch 135/1000\n",
      "2s - loss: 0.2919 - acc: 0.8868 - val_loss: 0.3262 - val_acc: 0.8670\n",
      "Epoch 136/1000\n",
      "1s - loss: 0.2920 - acc: 0.8857 - val_loss: 0.3325 - val_acc: 0.8680\n",
      "Epoch 137/1000\n",
      "1s - loss: 0.2910 - acc: 0.8852 - val_loss: 0.3307 - val_acc: 0.8610\n",
      "Epoch 138/1000\n",
      "1s - loss: 0.2900 - acc: 0.8884 - val_loss: 0.3685 - val_acc: 0.8355\n",
      "Epoch 139/1000\n",
      "1s - loss: 0.2911 - acc: 0.8882 - val_loss: 0.3213 - val_acc: 0.8700\n",
      "Epoch 140/1000\n",
      "2s - loss: 0.2901 - acc: 0.8850 - val_loss: 0.3204 - val_acc: 0.8675\n",
      "Epoch 141/1000\n",
      "2s - loss: 0.2903 - acc: 0.8864 - val_loss: 0.3221 - val_acc: 0.8680\n",
      "Epoch 142/1000\n",
      "2s - loss: 0.2895 - acc: 0.8884 - val_loss: 0.3298 - val_acc: 0.8570\n",
      "Epoch 143/1000\n",
      "1s - loss: 0.2901 - acc: 0.8846 - val_loss: 0.3330 - val_acc: 0.8635\n",
      "Epoch 144/1000\n",
      "2s - loss: 0.2901 - acc: 0.8870 - val_loss: 0.3238 - val_acc: 0.8645\n",
      "Epoch 145/1000\n",
      "2s - loss: 0.2885 - acc: 0.8911 - val_loss: 0.3198 - val_acc: 0.8775\n",
      "Epoch 146/1000\n",
      "2s - loss: 0.2883 - acc: 0.8864 - val_loss: 0.3336 - val_acc: 0.8690\n",
      "Epoch 147/1000\n",
      "2s - loss: 0.2880 - acc: 0.8884 - val_loss: 0.3290 - val_acc: 0.8565\n",
      "Epoch 148/1000\n",
      "2s - loss: 0.2883 - acc: 0.8913 - val_loss: 0.3293 - val_acc: 0.8650\n",
      "Epoch 149/1000\n",
      "2s - loss: 0.2880 - acc: 0.8897 - val_loss: 0.3204 - val_acc: 0.8705\n",
      "Epoch 150/1000\n",
      "2s - loss: 0.2850 - acc: 0.8918 - val_loss: 0.3252 - val_acc: 0.8610\n",
      "Epoch 151/1000\n",
      "2s - loss: 0.2885 - acc: 0.8895 - val_loss: 0.3592 - val_acc: 0.8575\n",
      "Epoch 152/1000\n",
      "2s - loss: 0.2861 - acc: 0.8895 - val_loss: 0.3314 - val_acc: 0.8740\n",
      "Epoch 153/1000\n",
      "2s - loss: 0.2867 - acc: 0.8870 - val_loss: 0.3227 - val_acc: 0.8635\n",
      "Epoch 154/1000\n",
      "2s - loss: 0.2867 - acc: 0.8857 - val_loss: 0.3349 - val_acc: 0.8680\n",
      "Epoch 155/1000\n",
      "2s - loss: 0.2855 - acc: 0.8884 - val_loss: 0.3519 - val_acc: 0.8525\n",
      "Epoch 156/1000\n",
      "1s - loss: 0.2873 - acc: 0.8868 - val_loss: 0.3192 - val_acc: 0.8715\n",
      "Epoch 157/1000\n",
      "1s - loss: 0.2857 - acc: 0.8924 - val_loss: 0.3433 - val_acc: 0.8615\n",
      "Epoch 158/1000\n",
      "1s - loss: 0.2859 - acc: 0.8915 - val_loss: 0.3240 - val_acc: 0.8635\n",
      "Epoch 159/1000\n",
      "1s - loss: 0.2844 - acc: 0.8911 - val_loss: 0.3412 - val_acc: 0.8475\n",
      "Epoch 160/1000\n",
      "1s - loss: 0.2837 - acc: 0.8906 - val_loss: 0.3502 - val_acc: 0.8675\n",
      "Epoch 161/1000\n",
      "1s - loss: 0.2842 - acc: 0.8911 - val_loss: 0.3297 - val_acc: 0.8595\n",
      "Epoch 162/1000\n",
      "2s - loss: 0.2869 - acc: 0.8877 - val_loss: 0.3185 - val_acc: 0.8775\n",
      "Epoch 163/1000\n",
      "1s - loss: 0.2803 - acc: 0.8958 - val_loss: 0.3615 - val_acc: 0.8495\n",
      "Epoch 164/1000\n",
      "1s - loss: 0.2859 - acc: 0.8877 - val_loss: 0.3327 - val_acc: 0.8565\n",
      "Epoch 165/1000\n",
      "1s - loss: 0.2840 - acc: 0.8933 - val_loss: 0.3385 - val_acc: 0.8615\n",
      "Epoch 166/1000\n",
      "1s - loss: 0.2853 - acc: 0.8918 - val_loss: 0.3233 - val_acc: 0.8785\n",
      "Epoch 167/1000\n",
      "1s - loss: 0.2834 - acc: 0.8915 - val_loss: 0.3302 - val_acc: 0.8645\n",
      "Epoch 168/1000\n",
      "1s - loss: 0.2825 - acc: 0.8886 - val_loss: 0.3241 - val_acc: 0.8640\n",
      "Epoch 169/1000\n",
      "1s - loss: 0.2829 - acc: 0.8900 - val_loss: 0.3250 - val_acc: 0.8770\n",
      "Epoch 170/1000\n",
      "1s - loss: 0.2819 - acc: 0.8931 - val_loss: 0.3232 - val_acc: 0.8790\n",
      "Epoch 171/1000\n",
      "1s - loss: 0.2828 - acc: 0.8891 - val_loss: 0.3372 - val_acc: 0.8555\n",
      "Epoch 172/1000\n",
      "1s - loss: 0.2795 - acc: 0.8895 - val_loss: 0.3402 - val_acc: 0.8590\n",
      "Epoch 173/1000\n",
      "1s - loss: 0.2844 - acc: 0.8922 - val_loss: 0.3254 - val_acc: 0.8605\n",
      "Epoch 174/1000\n",
      "1s - loss: 0.2812 - acc: 0.8933 - val_loss: 0.3261 - val_acc: 0.8595\n",
      "Epoch 175/1000\n",
      "1s - loss: 0.2815 - acc: 0.8900 - val_loss: 0.3329 - val_acc: 0.8590\n",
      "Epoch 176/1000\n",
      "1s - loss: 0.2827 - acc: 0.8900 - val_loss: 0.3213 - val_acc: 0.8685\n",
      "Epoch 177/1000\n",
      "1s - loss: 0.2823 - acc: 0.8861 - val_loss: 0.3239 - val_acc: 0.8620\n",
      "Epoch 178/1000\n",
      "1s - loss: 0.2838 - acc: 0.8924 - val_loss: 0.3354 - val_acc: 0.8560\n",
      "Epoch 179/1000\n",
      "2s - loss: 0.2823 - acc: 0.8918 - val_loss: 0.3248 - val_acc: 0.8575\n",
      "Epoch 180/1000\n",
      "2s - loss: 0.2824 - acc: 0.8913 - val_loss: 0.3382 - val_acc: 0.8510\n",
      "Epoch 181/1000\n",
      "2s - loss: 0.2818 - acc: 0.8906 - val_loss: 0.3191 - val_acc: 0.8735\n",
      "Epoch 182/1000\n",
      "2s - loss: 0.2806 - acc: 0.8918 - val_loss: 0.3321 - val_acc: 0.8650\n",
      "Epoch 183/1000\n",
      "1s - loss: 0.2798 - acc: 0.8938 - val_loss: 0.3440 - val_acc: 0.8595\n",
      "Epoch 184/1000\n",
      "1s - loss: 0.2782 - acc: 0.8886 - val_loss: 0.3228 - val_acc: 0.8680\n",
      "Epoch 185/1000\n",
      "1s - loss: 0.2786 - acc: 0.8922 - val_loss: 0.3437 - val_acc: 0.8410\n",
      "Epoch 186/1000\n",
      "1s - loss: 0.2799 - acc: 0.8922 - val_loss: 0.3269 - val_acc: 0.8620\n",
      "Epoch 187/1000\n",
      "1s - loss: 0.2789 - acc: 0.8897 - val_loss: 0.3321 - val_acc: 0.8555\n",
      "Epoch 188/1000\n",
      "1s - loss: 0.2825 - acc: 0.8882 - val_loss: 0.3265 - val_acc: 0.8615\n",
      "Epoch 189/1000\n",
      "1s - loss: 0.2777 - acc: 0.8918 - val_loss: 0.3341 - val_acc: 0.8570\n",
      "Epoch 190/1000\n",
      "1s - loss: 0.2806 - acc: 0.8913 - val_loss: 0.3202 - val_acc: 0.8750\n",
      "Epoch 191/1000\n",
      "1s - loss: 0.2799 - acc: 0.8931 - val_loss: 0.3253 - val_acc: 0.8600\n",
      "Epoch 192/1000\n",
      "1s - loss: 0.2785 - acc: 0.8947 - val_loss: 0.3260 - val_acc: 0.8685\n",
      "Epoch 193/1000\n",
      "1s - loss: 0.2786 - acc: 0.8927 - val_loss: 0.3235 - val_acc: 0.8710\n",
      "Epoch 194/1000\n",
      "1s - loss: 0.2782 - acc: 0.8913 - val_loss: 0.3250 - val_acc: 0.8615\n",
      "Epoch 195/1000\n",
      "1s - loss: 0.2782 - acc: 0.8938 - val_loss: 0.3331 - val_acc: 0.8765\n",
      "Epoch 196/1000\n",
      "1s - loss: 0.2793 - acc: 0.8933 - val_loss: 0.3279 - val_acc: 0.8585\n",
      "Epoch 197/1000\n",
      "1s - loss: 0.2768 - acc: 0.8933 - val_loss: 0.3183 - val_acc: 0.8785\n",
      "Epoch 198/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1s - loss: 0.2790 - acc: 0.8927 - val_loss: 0.3329 - val_acc: 0.8600\n",
      "Epoch 199/1000\n",
      "1s - loss: 0.2782 - acc: 0.8929 - val_loss: 0.3250 - val_acc: 0.8705\n",
      "Epoch 200/1000\n",
      "1s - loss: 0.2795 - acc: 0.8893 - val_loss: 0.3251 - val_acc: 0.8775\n",
      "Epoch 201/1000\n",
      "1s - loss: 0.2783 - acc: 0.8947 - val_loss: 0.3209 - val_acc: 0.8730\n",
      "Epoch 202/1000\n",
      "1s - loss: 0.2765 - acc: 0.8927 - val_loss: 0.3303 - val_acc: 0.8685\n",
      "Epoch 203/1000\n",
      "2s - loss: 0.2782 - acc: 0.8927 - val_loss: 0.3297 - val_acc: 0.8555\n",
      "Epoch 204/1000\n",
      "2s - loss: 0.2766 - acc: 0.8936 - val_loss: 0.3214 - val_acc: 0.8700\n",
      "Epoch 205/1000\n",
      "2s - loss: 0.2782 - acc: 0.8927 - val_loss: 0.3276 - val_acc: 0.8575\n",
      "Epoch 206/1000\n",
      "2s - loss: 0.2770 - acc: 0.8963 - val_loss: 0.3323 - val_acc: 0.8700\n",
      "Epoch 207/1000\n",
      "3s - loss: 0.2760 - acc: 0.8918 - val_loss: 0.3242 - val_acc: 0.8695\n",
      "Epoch 208/1000\n",
      "2s - loss: 0.2778 - acc: 0.8949 - val_loss: 0.3361 - val_acc: 0.8580\n",
      "Epoch 209/1000\n",
      "2s - loss: 0.2777 - acc: 0.8911 - val_loss: 0.3217 - val_acc: 0.8750\n",
      "Epoch 210/1000\n",
      "2s - loss: 0.2754 - acc: 0.8976 - val_loss: 0.3476 - val_acc: 0.8490\n",
      "Epoch 211/1000\n",
      "2s - loss: 0.2777 - acc: 0.8902 - val_loss: 0.3198 - val_acc: 0.8740\n",
      "Epoch 212/1000\n",
      "2s - loss: 0.2752 - acc: 0.8900 - val_loss: 0.3206 - val_acc: 0.8650\n",
      "Epoch 213/1000\n",
      "2s - loss: 0.2773 - acc: 0.8911 - val_loss: 0.3216 - val_acc: 0.8640\n",
      "Epoch 214/1000\n",
      "2s - loss: 0.2787 - acc: 0.8929 - val_loss: 0.3250 - val_acc: 0.8600\n",
      "Epoch 215/1000\n",
      "2s - loss: 0.2770 - acc: 0.8927 - val_loss: 0.3264 - val_acc: 0.8625\n",
      "Epoch 216/1000\n",
      "2s - loss: 0.2765 - acc: 0.8918 - val_loss: 0.3334 - val_acc: 0.8635\n",
      "Epoch 217/1000\n",
      "2s - loss: 0.2755 - acc: 0.8933 - val_loss: 0.3383 - val_acc: 0.8505\n",
      "Epoch 218/1000\n",
      "1s - loss: 0.2754 - acc: 0.8936 - val_loss: 0.3259 - val_acc: 0.8595\n",
      "Epoch 219/1000\n",
      "2s - loss: 0.2747 - acc: 0.8920 - val_loss: 0.3239 - val_acc: 0.8615\n",
      "Epoch 220/1000\n",
      "2s - loss: 0.2743 - acc: 0.8927 - val_loss: 0.3284 - val_acc: 0.8600\n",
      "Epoch 221/1000\n",
      "2s - loss: 0.2756 - acc: 0.8938 - val_loss: 0.3284 - val_acc: 0.8700\n",
      "Epoch 222/1000\n",
      "2s - loss: 0.2743 - acc: 0.8913 - val_loss: 0.3406 - val_acc: 0.8625\n",
      "Epoch 223/1000\n",
      "1s - loss: 0.2742 - acc: 0.8931 - val_loss: 0.3415 - val_acc: 0.8680\n",
      "Epoch 224/1000\n",
      "1s - loss: 0.2752 - acc: 0.8900 - val_loss: 0.3279 - val_acc: 0.8625\n",
      "Epoch 225/1000\n",
      "2s - loss: 0.2738 - acc: 0.8943 - val_loss: 0.3365 - val_acc: 0.8595\n",
      "Epoch 226/1000\n",
      "2s - loss: 0.2737 - acc: 0.8949 - val_loss: 0.3220 - val_acc: 0.8720\n",
      "Epoch 227/1000\n",
      "2s - loss: 0.2746 - acc: 0.8961 - val_loss: 0.3358 - val_acc: 0.8485\n",
      "Epoch 228/1000\n",
      "2s - loss: 0.2734 - acc: 0.8938 - val_loss: 0.3369 - val_acc: 0.8495\n",
      "Epoch 229/1000\n",
      "2s - loss: 0.2748 - acc: 0.8929 - val_loss: 0.3236 - val_acc: 0.8685\n",
      "Epoch 230/1000\n",
      "1s - loss: 0.2715 - acc: 0.8949 - val_loss: 0.3302 - val_acc: 0.8545\n",
      "Epoch 231/1000\n",
      "2s - loss: 0.2745 - acc: 0.8900 - val_loss: 0.3355 - val_acc: 0.8675\n",
      "Epoch 232/1000\n",
      "1s - loss: 0.2737 - acc: 0.8954 - val_loss: 0.3180 - val_acc: 0.8680\n",
      "Epoch 233/1000\n",
      "1s - loss: 0.2716 - acc: 0.8958 - val_loss: 0.3335 - val_acc: 0.8545\n",
      "Epoch 234/1000\n",
      "1s - loss: 0.2738 - acc: 0.8922 - val_loss: 0.3189 - val_acc: 0.8670\n",
      "Epoch 235/1000\n",
      "1s - loss: 0.2747 - acc: 0.8931 - val_loss: 0.3250 - val_acc: 0.8640\n",
      "Epoch 236/1000\n",
      "1s - loss: 0.2740 - acc: 0.8924 - val_loss: 0.3230 - val_acc: 0.8590\n",
      "Epoch 237/1000\n",
      "1s - loss: 0.2754 - acc: 0.8952 - val_loss: 0.3286 - val_acc: 0.8755\n",
      "Epoch 238/1000\n",
      "1s - loss: 0.2743 - acc: 0.8936 - val_loss: 0.3294 - val_acc: 0.8605\n",
      "Epoch 239/1000\n",
      "2s - loss: 0.2717 - acc: 0.8967 - val_loss: 0.3318 - val_acc: 0.8665\n",
      "Epoch 240/1000\n",
      "1s - loss: 0.2712 - acc: 0.8933 - val_loss: 0.3215 - val_acc: 0.8575\n",
      "Epoch 241/1000\n",
      "1s - loss: 0.2724 - acc: 0.8967 - val_loss: 0.3259 - val_acc: 0.8660\n",
      "Epoch 242/1000\n",
      "1s - loss: 0.2729 - acc: 0.8933 - val_loss: 0.3328 - val_acc: 0.8675\n",
      "Epoch 243/1000\n",
      "1s - loss: 0.2747 - acc: 0.8924 - val_loss: 0.3408 - val_acc: 0.8565\n",
      "Epoch 244/1000\n",
      "1s - loss: 0.2720 - acc: 0.8970 - val_loss: 0.3419 - val_acc: 0.8720\n",
      "Epoch 245/1000\n",
      "1s - loss: 0.2720 - acc: 0.8945 - val_loss: 0.3239 - val_acc: 0.8685\n",
      "Epoch 246/1000\n",
      "1s - loss: 0.2728 - acc: 0.8929 - val_loss: 0.3225 - val_acc: 0.8570\n",
      "Epoch 247/1000\n",
      "1s - loss: 0.2750 - acc: 0.8931 - val_loss: 0.3273 - val_acc: 0.8650\n",
      "Epoch 248/1000\n",
      "1s - loss: 0.2743 - acc: 0.8956 - val_loss: 0.3293 - val_acc: 0.8530\n",
      "Epoch 249/1000\n",
      "1s - loss: 0.2722 - acc: 0.8972 - val_loss: 0.3266 - val_acc: 0.8685\n",
      "Epoch 250/1000\n",
      "1s - loss: 0.2725 - acc: 0.8929 - val_loss: 0.3263 - val_acc: 0.8740\n",
      "Epoch 251/1000\n",
      "1s - loss: 0.2732 - acc: 0.8924 - val_loss: 0.3193 - val_acc: 0.8705\n",
      "Epoch 252/1000\n",
      "1s - loss: 0.2719 - acc: 0.8954 - val_loss: 0.3370 - val_acc: 0.8595\n",
      "Epoch 253/1000\n",
      "1s - loss: 0.2713 - acc: 0.8958 - val_loss: 0.3195 - val_acc: 0.8680\n",
      "Epoch 254/1000\n",
      "1s - loss: 0.2720 - acc: 0.8924 - val_loss: 0.3204 - val_acc: 0.8725\n",
      "Epoch 255/1000\n",
      "1s - loss: 0.2734 - acc: 0.8961 - val_loss: 0.3161 - val_acc: 0.8760\n",
      "Epoch 256/1000\n",
      "1s - loss: 0.2708 - acc: 0.8967 - val_loss: 0.3323 - val_acc: 0.8605\n",
      "Epoch 257/1000\n",
      "1s - loss: 0.2709 - acc: 0.8970 - val_loss: 0.3312 - val_acc: 0.8695\n",
      "Epoch 258/1000\n",
      "1s - loss: 0.2720 - acc: 0.8965 - val_loss: 0.3249 - val_acc: 0.8630\n",
      "Epoch 259/1000\n",
      "1s - loss: 0.2728 - acc: 0.8983 - val_loss: 0.3269 - val_acc: 0.8635\n",
      "Epoch 260/1000\n",
      "2s - loss: 0.2721 - acc: 0.8952 - val_loss: 0.3198 - val_acc: 0.8690\n",
      "Epoch 261/1000\n",
      "1s - loss: 0.2729 - acc: 0.8949 - val_loss: 0.3213 - val_acc: 0.8640\n",
      "Epoch 262/1000\n",
      "1s - loss: 0.2702 - acc: 0.8938 - val_loss: 0.3187 - val_acc: 0.8705\n",
      "Epoch 263/1000\n",
      "1s - loss: 0.2721 - acc: 0.8918 - val_loss: 0.3256 - val_acc: 0.8630\n",
      "Epoch 264/1000\n",
      "1s - loss: 0.2721 - acc: 0.8909 - val_loss: 0.3202 - val_acc: 0.8670\n",
      "Epoch 265/1000\n"
     ]
    }
   ],
   "source": [
    "# question 1 and question 2:\n",
    "decay = 1e-6\n",
    "learning_rate = 0.01\n",
    "epoch_num = 1000\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "batch_sizes = [4,8,16,32,64]\n",
    "for batch_size in batch_sizes:\n",
    "    bt = BatchTime()\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(10, activation='relu')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks=[bt])\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "    \n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error and the test accuracy against number of epochs\n",
    "    plt.figure()\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    plt.savefig(plot_pic_folder + '2/p1a_batchsize' + str(batch_size) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('test accuracy')\n",
    "\n",
    "    plt.savefig(plot_pic_folder + '2/p1a_batchsize' + str(batch_size) + '_sample_accuracy.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(batch_sizes, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.xticks(batch_sizes)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '2/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(batch_sizes, total_time_taken, label='time_taken')\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.xticks(batch_sizes)\n",
    "plt.ylabel('time_taken(s)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '2/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "decay = 1e-6\n",
    "learning_rate = 0.01\n",
    "epoch_num = 1000\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "neuron_nums = [5,10,15,20,25]\n",
    "for neuron_num in neuron_nums:\n",
    "    bt = BatchTime()\n",
    "\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(neuron_num, activation='sigmoid')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          callbacks=[bt],\n",
    "                          batch_size = 32)\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error and the test accuracy against number of epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.savefig(plot_pic_folder + '3/p1a_neuron' + str(neuron_num) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('test accuracy')\n",
    "    annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "    plt.savefig(plot_pic_folder + '3/p1a_neuron' + str(neuron_num) + '_sample_accuracy.png')\n",
    "    plt.close()\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(neuron_nums, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('neuron_nums')\n",
    "plt.xticks(neuron_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '3/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(neuron_nums, total_time_taken, label='time_taken')\n",
    "plt.xlabel('neuron_nums')\n",
    "plt.xticks(neuron_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '3/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#question 4\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch_num = 1000\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "\n",
    "decay_nums = [0,math.pow(10,-3),math.pow(10,-6),math.pow(10,-9),math.pow(10,-9)]\n",
    "test_accuracy = []\n",
    "for decay in decay_nums:\n",
    "    bt = BatchTime()\n",
    "    start_time = time.time()\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(10, activation='sigmoid')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          callbacks=[bt],\n",
    "                          batch_size = 32)\n",
    "\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error against number of epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.savefig(plot_pic_folder + '4/p1a_decay' + str(decay) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    test_accuracy.append(max(train_his.history['val_acc']))\n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "    \n",
    "#Plot the test accuracy against the different values of decay parameter.\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, test_accuracy, label='test accuracy')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test accuracy')\n",
    "annot_max(decay_nums, np.array(test_accuracy))\n",
    "plt.savefig(plot_pic_folder + '4/p1a_test_accuracy.png')\n",
    "plt.close()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "    \n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.xticks(decay_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '4/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, total_time_taken, label='time_taken')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.xticks(decay_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '4/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# design a 4-layer network with two hidden- layers, \n",
    "# each consisting of 10 neurons with logistic activation functions, \n",
    "# batch size of 32 and decay parameter 10-6.\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch_num = 1000\n",
    "decay = math.pow(10,-6)\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "\n",
    "bt = BatchTime()\n",
    "start_time = time.time()\n",
    "inputs = Input(shape=(trainX.shape[1],))\n",
    "dense1 = Dense(10, activation='sigmoid')(inputs)\n",
    "dense2 = Dense(10, activation='sigmoid')(dense1)\n",
    "predictions = Dense(trainY.shape[1], activation='softmax')(dense2)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                      validation_data=[testX,testY],\n",
    "                      callbacks=[bt],\n",
    "                      batch_size = 32)\n",
    "\n",
    "print(\"Time taken to train a 4-layer network is : \" + str(time.time() - start_time))\n",
    "    \n",
    "#  Plot the train and test accuracy of the 4-layer network.\n",
    "# Plot the training error and the test accuracy against number of epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cross-entropy')\n",
    "plt.title('training cost')\n",
    "annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "plt.savefig(plot_pic_folder + '5/p1a_cost.png')\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test accuracy')\n",
    "annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "plt.savefig(plot_pic_folder + '5/p1a_sample_accuracy.png')\n",
    "plt.close()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "# b) Compare and comment on the performances on 3-layer and 4-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
