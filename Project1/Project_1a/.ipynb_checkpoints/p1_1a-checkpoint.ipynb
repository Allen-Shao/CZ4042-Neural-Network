{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/opt/python/bin/python2.7'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name np_utils",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a91080163d89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name np_utils"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras import optimizers, callbacks\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from keras.utils import np_utils\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_pic_folder = \"./pic/\"\n",
    "#  create folder if not exists\n",
    "if not os.path.exists(plot_pic_folder):\n",
    "    os.makedirs(plot_pic_folder)\n",
    "num_of_question = 5\n",
    "for i in range(1,num_of_question+1):\n",
    "    if not os.path.exists(plot_pic_folder + str(i) + '/'):\n",
    "        os.makedirs(plot_pic_folder + str(i) + '/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchTime(callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.logs=[]\n",
    "        self.batch_time = []\n",
    "        self.start_time = []\n",
    "        self.cur_start_time = 0\n",
    "        \n",
    "        \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        self.cur_start_time = time.time()\n",
    "        self.start_time.append(self.cur_start_time)\n",
    "        return\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.batch_time.append(time.time() - self.cur_start_time)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale data\n",
    "def scale(X, X_min, X_max):\n",
    "    # min-max normalization\n",
    "    return (X - X_min)/(X_max-np.min(X, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_data (samples, labels):\n",
    "    idx = np.arange(samples.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    #print  (samples.shape, labels.shape)\n",
    "    samples, labels = samples[idx], labels[idx]\n",
    "    return samples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Point out the maximum and minimum point\n",
    "\n",
    "def annot_max(x,y, ax=None):\n",
    "    xmax = x[np.argmax(y)]\n",
    "    ymax = y.max()\n",
    "    text= \"MAX Point x={:.3f}, y={:.3f}\".format(xmax, ymax)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=60\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"top\")\n",
    "    ax.annotate(text, xy=(xmax, ymax), xytext=(0.94,0.96), **kw)\n",
    "\n",
    "def annot_min(x,y, ax=None):\n",
    "    xmin = x[np.argmin(y)]\n",
    "    ymin = y.min()\n",
    "    text= \"MIN Point x={:.3f}, y={:.3f}\".format(xmin, ymin)\n",
    "    if not ax:\n",
    "        ax=plt.gca()\n",
    "    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "    arrowprops=dict(arrowstyle=\"->\",connectionstyle=\"angle,angleA=0,angleB=120\")\n",
    "    kw = dict(xycoords='data',textcoords=\"axes fraction\",\n",
    "              arrowprops=arrowprops, bbox=bbox_props, ha=\"right\", va=\"bottom\")\n",
    "    ax.annotate(text, xy=(xmin, ymin), xytext=(0.94,0.96), **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4435, 36) (4435, 6)\n",
      "(2000, 36) (2000, 6)\n"
     ]
    }
   ],
   "source": [
    "#read train data\n",
    "train_input = np.loadtxt('./data/sat_train.txt',delimiter=' ')\n",
    "trainX, train_Y = train_input[:,:36], train_input[:,-1].astype(int)\n",
    "trainX_min, trainX_max = np.min(trainX, axis=0), np.max(trainX, axis=0)\n",
    "trainX = scale(trainX, trainX_min, trainX_max)\n",
    "\n",
    "train_Y[train_Y == 7] = 6\n",
    "trainY = np.zeros((train_Y.shape[0], 6))\n",
    "trainY[np.arange(train_Y.shape[0]), train_Y-1] = 1\n",
    "\n",
    "\n",
    "#read test data\n",
    "test_input = np.loadtxt('./data/sat_test.txt',delimiter=' ')\n",
    "testX, test_Y = test_input[:,:36], test_input[:,-1].astype(int)\n",
    "\n",
    "# testX_min, testX_max = np.min(testX, axis=0), np.max(testX, axis=0)\n",
    "testX = scale(testX, trainX_min, trainX_max)\n",
    "\n",
    "test_Y[test_Y == 7] = 6\n",
    "testY = np.zeros((test_Y.shape[0], 6))\n",
    "testY[np.arange(test_Y.shape[0]), test_Y-1] = 1\n",
    "\n",
    "del test_Y, train_Y, trainX_min, trainX_max\n",
    "print(trainX.shape, trainY.shape)\n",
    "print(testX.shape, testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "4s - loss: 1.4201 - acc: 0.5112 - val_loss: 1.1223 - val_acc: 0.6520\n",
      "Epoch 2/5\n",
      "3s - loss: 0.9166 - acc: 0.7227 - val_loss: 0.8297 - val_acc: 0.7245\n",
      "Epoch 3/5\n",
      "3s - loss: 0.7146 - acc: 0.7865 - val_loss: 0.6811 - val_acc: 0.7675\n",
      "Epoch 4/5\n",
      "3s - loss: 0.5982 - acc: 0.8074 - val_loss: 0.5990 - val_acc: 0.7795\n",
      "Epoch 5/5\n",
      "3s - loss: 0.5252 - acc: 0.8178 - val_loss: 0.5256 - val_acc: 0.8045\n",
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "2s - loss: 1.6986 - acc: 0.2388 - val_loss: 1.6196 - val_acc: 0.2630\n",
      "Epoch 2/5\n",
      "1s - loss: 1.5018 - acc: 0.3813 - val_loss: 1.4271 - val_acc: 0.4790\n",
      "Epoch 3/5\n",
      "1s - loss: 1.2971 - acc: 0.5853 - val_loss: 1.2272 - val_acc: 0.6920\n",
      "Epoch 4/5\n",
      "1s - loss: 1.1040 - acc: 0.7105 - val_loss: 1.0355 - val_acc: 0.7090\n",
      "Epoch 5/5\n",
      "1s - loss: 0.9173 - acc: 0.7448 - val_loss: 0.8734 - val_acc: 0.7230\n",
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "1s - loss: 1.6693 - acc: 0.2390 - val_loss: 1.5816 - val_acc: 0.2380\n",
      "Epoch 2/5\n",
      "0s - loss: 1.4865 - acc: 0.4311 - val_loss: 1.4493 - val_acc: 0.5325\n",
      "Epoch 3/5\n",
      "0s - loss: 1.3491 - acc: 0.6541 - val_loss: 1.3090 - val_acc: 0.7085\n",
      "Epoch 4/5\n",
      "0s - loss: 1.1921 - acc: 0.7116 - val_loss: 1.1449 - val_acc: 0.7030\n",
      "Epoch 5/5\n",
      "0s - loss: 1.0340 - acc: 0.7317 - val_loss: 1.0040 - val_acc: 0.7240\n",
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "1s - loss: 1.6854 - acc: 0.2266 - val_loss: 1.6117 - val_acc: 0.2230\n",
      "Epoch 2/5\n",
      "0s - loss: 1.5267 - acc: 0.3865 - val_loss: 1.5027 - val_acc: 0.4610\n",
      "Epoch 3/5\n",
      "0s - loss: 1.4229 - acc: 0.5130 - val_loss: 1.4032 - val_acc: 0.5280\n",
      "Epoch 4/5\n",
      "0s - loss: 1.3253 - acc: 0.5795 - val_loss: 1.3144 - val_acc: 0.5805\n",
      "Epoch 5/5\n",
      "0s - loss: 1.2390 - acc: 0.6410 - val_loss: 1.2344 - val_acc: 0.6655\n",
      "Train on 4435 samples, validate on 2000 samples\n",
      "Epoch 1/5\n",
      "0s - loss: 1.7487 - acc: 0.2327 - val_loss: 1.6719 - val_acc: 0.2280\n",
      "Epoch 2/5\n",
      "0s - loss: 1.6023 - acc: 0.3871 - val_loss: 1.6001 - val_acc: 0.4615\n",
      "Epoch 3/5\n",
      "0s - loss: 1.5406 - acc: 0.5750 - val_loss: 1.5468 - val_acc: 0.5630\n",
      "Epoch 4/5\n",
      "0s - loss: 1.4885 - acc: 0.6000 - val_loss: 1.4978 - val_acc: 0.5850\n",
      "Epoch 5/5\n",
      "0s - loss: 1.4385 - acc: 0.6115 - val_loss: 1.4482 - val_acc: 0.6625\n"
     ]
    }
   ],
   "source": [
    "# question 1 and question 2:\n",
    "decay = 1e-6\n",
    "learning_rate = 0.01\n",
    "epoch_num = 5\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "batch_sizes = [4,8,16,32,64]\n",
    "for batch_size in batch_sizes:\n",
    "    bt = BatchTime()\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(10, activation='relu')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    # This creates a model that includes\n",
    "    # the Input layer and three Dense layers\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks=[bt])\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "    \n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error and the test accuracy against number of epochs\n",
    "    plt.figure()\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    plt.savefig(plot_pic_folder + '2/p1a_batchsize' + str(batch_size) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('test accuracy')\n",
    "\n",
    "    plt.savefig(plot_pic_folder + '2/p1a_batchsize' + str(batch_size) + '_sample_accuracy.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(batch_sizes, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.xticks(batch_sizes)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '2/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(batch_sizes, total_time_taken, label='time_taken')\n",
    "plt.xlabel('batch_sizes')\n",
    "plt.xticks(batch_sizes)\n",
    "plt.ylabel('time_taken(s)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '2/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "decay = 1e-6\n",
    "learning_rate = 0.01\n",
    "epoch_num = 2\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "neuron_nums = [5,10,15,20,25]\n",
    "for neuron_num in neuron_nums:\n",
    "    bt = BatchTime()\n",
    "\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(neuron_num, activation='sigmoid')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          callbacks=[bt],\n",
    "                          batch_size = 32)\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error and the test accuracy against number of epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.savefig(plot_pic_folder + '3/p1a_neuron' + str(neuron_num) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title('test accuracy')\n",
    "    annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "    plt.savefig(plot_pic_folder + '3/p1a_neuron' + str(neuron_num) + '_sample_accuracy.png')\n",
    "    plt.close()\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(neuron_nums, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('neuron_nums')\n",
    "plt.xticks(neuron_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '3/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(neuron_nums, total_time_taken, label='time_taken')\n",
    "plt.xlabel('neuron_nums')\n",
    "plt.xticks(neuron_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '3/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#question 4\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch_num = 2\n",
    "\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "\n",
    "decay_nums = [0,math.pow(10,-3),math.pow(10,-6),math.pow(10,-9),math.pow(10,-9)]\n",
    "test_accuracy = []\n",
    "for decay in decay_nums:\n",
    "    bt = BatchTime()\n",
    "    start_time = time.time()\n",
    "    inputs = Input(shape=(trainX.shape[1],))\n",
    "    dense1 = Dense(10, activation='sigmoid')(inputs)\n",
    "    predictions = Dense(trainY.shape[1], activation='softmax')(dense1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "    model.compile(optimizer=sgd,\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    start_time = time.time()\n",
    "    train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                          validation_data=[testX,testY],\n",
    "                          callbacks=[bt],\n",
    "                          batch_size = 32)\n",
    "\n",
    "    total_time_taken.append(time.time() - start_time)\n",
    "#     loss, acc = model.evaluate(testX, testY, verbose=2)\n",
    "\n",
    "    \n",
    "    # Plot the training error against number of epochs\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "    plt.xlabel('iterations')\n",
    "    plt.ylabel('cross-entropy')\n",
    "    plt.title('training cost')\n",
    "    annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "    plt.savefig(plot_pic_folder + '4/p1a_decay' + str(decay) + '_cost.png')\n",
    "    plt.gcf().clear()\n",
    "    plt.close()\n",
    "    \n",
    "    test_accuracy.append(max(train_his.history['val_acc']))\n",
    "    batch_time_taken.append(np.mean(bt.batch_time))\n",
    "    \n",
    "#Plot the test accuracy against the different values of decay parameter.\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, test_accuracy, label='test accuracy')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test accuracy')\n",
    "annot_max(decay_nums, np.array(test_accuracy))\n",
    "plt.savefig(plot_pic_folder + '4/p1a_test_accuracy.png')\n",
    "plt.close()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "    \n",
    "\n",
    "# Plot the time taken\n",
    "batch_time_taken = np.array(batch_time_taken) * 1000\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, batch_time_taken, label='time_taken')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.xticks(decay_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('time for a weight update')\n",
    "plt.savefig(plot_pic_folder + '4/p1a_batch_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(decay_nums, total_time_taken, label='time_taken')\n",
    "plt.xlabel('decay_nums')\n",
    "plt.xticks(decay_nums)\n",
    "plt.ylabel('time_taken(ms)')\n",
    "plt.title('total time for training')\n",
    "plt.savefig(plot_pic_folder + '4/p1a_total_time.png')\n",
    "# plt.show()\n",
    "plt.gcf().clear()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question 5\n",
    "# design a 4-layer network with two hidden- layers, \n",
    "# each consisting of 10 neurons with logistic activation functions, \n",
    "# batch size of 32 and decay parameter 10-6.\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "epoch_num = 1000\n",
    "decay = math.pow(10,-6)\n",
    "batch_time_taken = []\n",
    "total_time_taken = []\n",
    "\n",
    "bt = BatchTime()\n",
    "start_time = time.time()\n",
    "inputs = Input(shape=(trainX.shape[1],))\n",
    "dense1 = Dense(10, activation='sigmoid')(inputs)\n",
    "dense2 = Dense(10, activation='sigmoid')(dense1)\n",
    "predictions = Dense(trainY.shape[1], activation='softmax')(dense2)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "sgd = optimizers.SGD(lr=learning_rate, decay=decay)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "start_time = time.time()\n",
    "train_his = model.fit(trainX, trainY, epochs=epoch_num, verbose=2, \n",
    "                      validation_data=[testX,testY],\n",
    "                      callbacks=[bt],\n",
    "                      batch_size = 32)\n",
    "\n",
    "print(\"Time taken to train a 4-layer network is : \" + str(time.time() - start_time))\n",
    "    \n",
    "#  Plot the train and test accuracy of the 4-layer network.\n",
    "# Plot the training error and the test accuracy against number of epochs\n",
    "plt.figure()\n",
    "plt.plot(range(1, 1+epoch_num), train_his.history['loss'], label='train_cost')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('cross-entropy')\n",
    "plt.title('training cost')\n",
    "annot_min(range(1, 1+epoch_num), np.array(train_his.history['loss']))\n",
    "plt.savefig(plot_pic_folder + '5/p1a_cost.png')\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(1, 1+epoch_num), train_his.history['val_acc'], label='test_accuracy')\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('test accuracy')\n",
    "annot_max(range(1, 1+epoch_num), np.array(train_his.history['val_acc']))\n",
    "plt.savefig(plot_pic_folder + '5/p1a_sample_accuracy.png')\n",
    "plt.close()\n",
    "plt.gcf().clear()\n",
    "plt.close()\n",
    "# b) Compare and comment on the performances on 3-layer and 4-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
